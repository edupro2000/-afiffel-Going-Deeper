{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "isolated-boring",
   "metadata": {},
   "source": [
    "# 프로젝트: SentencePiece 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-panic",
   "metadata": {},
   "source": [
    "## Step 1. SentencePiece 설치하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "random-irish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (0.1.95)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-highland",
   "metadata": {},
   "source": [
    "## Step 2. SentencePiece 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "earlier-citizen",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filtered_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-64cdeba275be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiltered_corpus\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# 이전 스텝에서 정제했던 corpus를 활용합니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filtered_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "temp_file = os.getenv('HOME')+'/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp'\n",
    "\n",
    "vocab_size = 8000\n",
    "\n",
    "with open(temp_file, 'w') as f:\n",
    "    for row in filtered_corpus:   # 이전 스텝에서 정제했던 corpus를 활용합니다.\n",
    "        f.write(str(row) + '\\n')\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    '--input={} --model_prefix=korean_spm --vocab_size={}'.format(temp_file, vocab_size)    \n",
    ")\n",
    "#위 Train에서  --model_type = 'unigram'이 디폴트 적용되어 있습니다. --model_type = 'bpe' 로 옵션을 주어 변경할 수 있습니다.\n",
    "\n",
    "!ls -l korean_spm*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-knife",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = spm.SentencePieceProcessor()\n",
    "s.Load('korean_spm.model')\n",
    "\n",
    "# SentencePiece를 활용한 sentence -> encoding\n",
    "tokensIDs = s.EncodeAsIds('아버지가방에들어가신다.')\n",
    "print(tokensIDs)\n",
    "\n",
    "# SentencePiece를 활용한 sentence -> encoded pieces\n",
    "print(s.SampleEncodeAsPieces('아버지가방에들어가신다.',1, 0.0))\n",
    "\n",
    "# SentencePiece를 활용한 encoding -> sentence 복원\n",
    "print(s.DecodeIds(tokensIDs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-cover",
   "metadata": {},
   "source": [
    "## Step 3. Tokenizer 함수 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-tucson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp_tokenize(s, corpus):\n",
    "\n",
    "    tensor = []\n",
    "\n",
    "    for sen in corpus:\n",
    "        tensor.append(s.EncodeAsIds(sen))\n",
    "\n",
    "    with open(\"./korean_spm.vocab\", 'r') as f:\n",
    "        vocab = f.readlines()\n",
    "\n",
    "    word_index = {}\n",
    "    index_word = {}\n",
    "\n",
    "    for idx, line in enumerate(vocab):\n",
    "        word = line.split(\"\\t\")[0]\n",
    "\n",
    "        word_index.update({idx:word})\n",
    "        index_word.update({word:idx})\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "    return tensor, word_index, index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-provision",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sp_tokenize(s, corpus) 사용예제\n",
    "\n",
    "my_corpus = ['나는 밥을 먹었습니다.', '그러나 여전히 ㅠㅠ 배가 고픕니다...']\n",
    "tensor, word_index, index_word = sp_tokenize(s, my_corpus)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-diving",
   "metadata": {},
   "source": [
    "### Step 4. 네이버 영화리뷰 감정 분석 문제에 SentencePiece 적용해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-cornell",
   "metadata": {},
   "outputs": [],
   "source": [
    "#폴더생성\n",
    "!mkdir ~/aiffel/sp_tokenizer/sentiment_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-fruit",
   "metadata": {},
   "outputs": [],
   "source": [
    "#리뷰데이터 받아오기\n",
    "!wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt  \n",
    "!wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\n",
    "!mv ratings_*.txt ~/aiffel/sp_tokenizer/sentiment_classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-operation",
   "metadata": {},
   "source": [
    "[추가 학습]\n",
    "- ! wget + DOWNLOAD-URL  : 단일 파일받기\n",
    "- ! mv 파일명 + 이동경로 : 하나 이상의 파일이나 디렉터리를 한 장소에서 한 다른 장소로 이동하도록 만드는 유닉스 명령어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reverse-marketing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터 준비와 확인\n",
    "\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "\n",
    "# 데이터를 읽기\n",
    "train_data = pd.read_table('~/aiffel/sp_tokenizer/sentiment_classification/ratings_train.txt') #경로 변경 완료\n",
    "test_data = pd.read_table('~/aiffel/sp_tokenizer/sentiment_classification/ratings_test.txt')   #경로 변경 완료\n",
    "\n",
    "display(train_data.head(6))\n",
    "display(test_data.head(6))\n",
    "print(\"훈련 데이터 개수 : {}, 테스트 데이터 개수: {}\".format(len(train_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-belgium",
   "metadata": {},
   "source": [
    "#### 1) 학습  및 테스트 데이터 중복 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-isolation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "test_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "\n",
    "print('중복제거 후 학습데이터 문장의 개수 : ',len(train_data['document']))\n",
    "print('중복제거 후 테스트데이터 문장의 개수 : ',len(test_data['document']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-celtic",
   "metadata": {},
   "source": [
    "#### 2) 학습 및 테스트 데이터 Nan 결측치 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "young-regulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.dropna(how = 'any')\n",
    "test_data =test_data.dropna(how='any')\n",
    "\n",
    "print('결측치제거 후 학습데이터 문장의 개수 : ',len(train_data['document']))\n",
    "print('결측치제거 후 테스트데이터 문장의 개수 : ',len(test_data['document']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-chuck",
   "metadata": {},
   "source": [
    "[중복제거]\n",
    "\n",
    "- ex 노드는 중복값  drop_duplicates 메서드 이용하여 제거 , train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "\n",
    "- nlp 노드는 중복제거시 cleaned_corpus = list(set(raw))  # set를 사용해서 중복을 제거합니다.\n",
    "    - set은 집합을 정의하는 자료형인데, 중복을 허용하지 않아 변환 과정에서 자동으로 중복된 요소를 제거\n",
    "    - 대신 list의 순서가 뒤죽박죽될 수 있으니, 만약 번역 데이터처럼 쌍을 이뤄야 하는 경우라면 주의해서 사용\n",
    "\n",
    "좀 햇갈리는 부분 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-extent",
   "metadata": {},
   "source": [
    "#### 3) 문장 corpus 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-harvey",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  문장의 최단 길이, 최장 길이, 평균 길이를 구한 후 문장 길이 분포를 막대그래프로 표현해 주는 소스\n",
    "\n",
    "min_len = 999\n",
    "max_len = 0\n",
    "sum_len = 0\n",
    "\n",
    "for sen in train_data['document']:  # train_data['document'] 앞서 다운로드받은 데이터가 담긴 변수\n",
    "    length = len(sen)\n",
    "    if min_len > length: min_len = length\n",
    "    if max_len < length: max_len = length\n",
    "    sum_len += length\n",
    "\n",
    "print(\"Data Size:\", len(train_data['document']))\n",
    "print(\"문장의 최단 길이:\", min_len)\n",
    "print(\"문장의 최장 길이:\", max_len)\n",
    "print(\"문장의 평균 길이:\", sum_len // len(train_data['document']))\n",
    "\n",
    "sentence_length = np.zeros((max_len), dtype=np.int) # np.zeros 주어진 형태와 타입을 갖는 0으로 채워진 어레이를 반환\n",
    "\n",
    "for sen in train_data['document']:   \n",
    "    sentence_length[len(sen)-1] += 1\n",
    "\n",
    "plt.bar(range(max_len), sentence_length, width=1.0)\n",
    "plt.title(\"train_data Sentence Length Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-mountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정제된 데이터를 공백 기반으로 토큰화하여 저장\n",
    "\n",
    "cleaned_corpus = []\n",
    "for sen in train_data['document']:\n",
    "    cleaned_corpus.append(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-milton",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 60\n",
    "min_len = 5 # 너무 짧은 데이터는 노이즈로 작용할수 있으므로 길이 5미만 제거\n",
    "\n",
    "# 길이 조건에 맞는 문장만 선택합니다.\n",
    "filtered_corpus = [s for s in cleaned_corpus if (len(s) < max_len) & (len(s) >= min_len)]\n",
    "\n",
    "# 분포도를 다시 그려봅니다.\n",
    "sentence_length = np.zeros((max_len), dtype=np.int)\n",
    "\n",
    "for sen in filtered_corpus:\n",
    "    sentence_length[len(sen)-1] += 1\n",
    "\n",
    "plt.bar(range(max_len), sentence_length, width=1.0)\n",
    "plt.title(\"Sentence Length Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-juice",
   "metadata": {},
   "source": [
    "### 1) 네이버 영화리뷰 감정 분석 코퍼스에 SentencePiece를 적용시킨 모델 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-virgin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "temp_file = os.getenv('HOME')+'/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp'\n",
    "\n",
    "vocab_size = 8000\n",
    "\n",
    "with open(temp_file, 'w') as f:\n",
    "    for row in filtered_corpus:   # 이전 스텝에서 정제했던 corpus를 활용합니다.\n",
    "        f.write(str(row) + '\\n')\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    '--input={} --model_prefix=korean_spm --vocab_size={}'.format(temp_file, vocab_size)    \n",
    ")\n",
    "#위 Train에서  --model_type = 'unigram'이 디폴트 적용되어 있습니다. --model_type = 'bpe' 로 옵션을 주어 변경할 수 있습니다.\n",
    "\n",
    "!ls -l korean_spm*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-manual",
   "metadata": {},
   "source": [
    "### 2) 학습된 모델로 sp_tokenize() 메소드 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-meter",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = spm.SentencePieceProcessor()\n",
    "print(s.Load('korean_spm.model'))\n",
    "\n",
    "def sp_tokenize(s, corpus):\n",
    "\n",
    "    tensor = []\n",
    "\n",
    "    for sen in corpus:\n",
    "        tensor.append(s.EncodeAsIds(sen))\n",
    "\n",
    "    with open(\"./korean_spm.vocab\", 'r') as f:\n",
    "        vocab = f.readlines()\n",
    "\n",
    "    word_index = {}\n",
    "    index_word = {}\n",
    "\n",
    "    for idx, line in enumerate(vocab):\n",
    "        word = line.split(\"\\t\")[0]\n",
    "\n",
    "        word_index.update({idx:word})\n",
    "        index_word.update({word:idx})\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "    return tensor, word_index, index_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-inventory",
   "metadata": {},
   "source": [
    "### 3) 구현된 토크나이저를 적용하여 네이버 영화리뷰 감정 분석 모델을 재학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-canberra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설계\n",
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 41  # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-stable",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) LSTM 모델\n",
    "model = keras.Sequential(name='LSTM')\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.LSTM(8))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경 가능)\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-surgery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_input, train_target, epochs=10, validation_data=(val_input, val_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-connectivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 테스트셋으로 평가  \n",
    "results = model.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-telescope",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-neutral",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-microphone",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-nylon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-hollow",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "passing-concert",
   "metadata": {},
   "source": [
    "#### KoNLPy 형태소 분석기를 사용한 모델과 성능 비교하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-panic",
   "metadata": {},
   "source": [
    "#### (보너스) SentencePiece 모델의 model_type, vocab_size 등을 변경해 가면서 성능 개선 여부 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-passing",
   "metadata": {},
   "outputs": [],
   "source": [
    "코퍼스 분석, 전처리, SentencePiece 적용, 토크나이저 구현 및 동작이 빠짐없이 진행되었는가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "SentencePiece 토크나이저가 적용된 Text Classifier 모델이 정상적으로 수렴하여 80% 이상의 test accuracy가 확인되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-latex",
   "metadata": {},
   "outputs": [],
   "source": [
    "SentencePiece 토크나이저를 활용했을 때의 성능을 다른 토크나이저 혹은 SentencePiece의 다른 옵션의 경우와 비교하여 분석을 체계적으로 진행하였다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-enemy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-snowboard",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-illinois",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-gross",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decent-frequency",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-spring",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-archive",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-shakespeare",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-trail",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-shipping",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-ocean",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-execution",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-islam",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-allocation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-obligation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-saint",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-smooth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-imperial",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-wallet",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-psychology",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-guitar",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-covering",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-visitor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-collapse",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-heater",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-crawford",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-morocco",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-sharp",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-shirt",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
