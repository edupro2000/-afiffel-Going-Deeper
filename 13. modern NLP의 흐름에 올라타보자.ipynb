{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "invalid-recipe",
   "metadata": {},
   "source": [
    "##  Transfer Learning과 Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-guarantee",
   "metadata": {},
   "source": [
    "- 전이 학습 : 특정 환경에서 학습을 마친 신경망(일부 혹은 전부)을 유사하거나 다른 환경에서 사용하는 것을 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-satellite",
   "metadata": {},
   "source": [
    "- fine-tuning :  주어진 문제(다운스트림 테스크 혹은 downstream task)를 잘 풀기 위해 pretrained model을 재학습시키는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-party",
   "metadata": {},
   "source": [
    "## ELMO(Embedding from Language Models)\n",
    "- ELMo는 문맥(context)을 반영한 임베딩을 pretrained model로 구현한 첫 번째 사례"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-chapel",
   "metadata": {},
   "source": [
    "### [ELMo는 세 가지 요소로 구성]\n",
    "\n",
    "- 1. character-level CNN : 입력된 문자들 간의 관계를 파악하고 임베딩 벡터로 변환하는 역할\n",
    "    - character-level 문자인식 > 유니코드 ID 입력 받기 > 단어의 시작과 끝 <BOW>와 <EOW>에 해당하는 유니코드를 앞뒤로 붙이기>look-up table\n",
    "        - Look up 테이블은 'Key-Value' 구조로 저장, '카테고리' 로 분류되는 정보 (예를 들어서 지역, 통화, 색상 등)의 정보를 담고 있다. \n",
    "    - 만들어진 벡터에 (n x 임베딩 차원 수) 사이즈의 필터로 컨볼루션하여 피처맵을 만들기>max-pooling하여 하나의 값을 뽑기 >반복\n",
    "- 2. bidirectional LSTM\n",
    "    - character-level CNN을 통과하여 만들어진 벡터들은 bidirectional(양방향 학습) LSTM을 통과> softmax로 다음에 올 단어들을 예측\n",
    "        - 주어진 입력을 한 번은 순방향으로, 한 번은 역방향으로 각각 2개의 LSTM layer를 통과하면 성능이 좋다.\n",
    "        - 순방향과 역방향의 벡터를 합치거나 더하지 않고 각각 독립적인 모델처럼 행동(서로 다른 방향의 모델에게 정답을 가르칠 가능성 배제)\n",
    "- 3. ELMO 임베딩 레이어\n",
    "    - 우선 구하려는 토큰에 대하여 각 층의 출력값( character-level CNN 을 통과한 후 나오는 벡터)을 가지고 온다 .\n",
    "    - 각층 마다 가중치 $S_j$ 를 곱해서 모두 더해준다. 마지막으로 다운스트림 태스크의 가중치 Y를 곱하면 ELMo의 임베딩이된다. \n",
    "    <mark>즉, 구하고자 하는 토큰에 대한 각 층의 출력값을 가중합한 것이 ELMo 임베딩이다.</mark>\n",
    "- 4. <a href=\"https://arxiv.org/abs/1802.05365\" target=\"_blank\"> 자료 읽어보기 (영문)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-poster",
   "metadata": {},
   "source": [
    "### GPT\n",
    "- 트랜스포머의 decoder를 아주 깊고 깊게 쌓아 많은 데이터를 학습 시켜 성능을 높힌 네트워크, ANSWKD TODTJDDP AODN XMRGHKEHLDJ DLtEK.\n",
    "     - Decoder 구성 :  masked Multi-Head Attention, Multi-Head Attention, Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-scientist",
   "metadata": {},
   "source": [
    "#### Transfomer Decoder Block : Pretraining LM (Unspervised Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-whale",
   "metadata": {},
   "source": [
    "##### 1)1. Embedding\n",
    "- GPT는 텍스트의 Embedding으로 BPE(Byte-pair Encoding)를 사용\n",
    "- BPE는 모든 단어를 문자(바이트)들의 집합으로 취급하여 자주 등장하는 문자 쌍을 합치는 subword tokenization이었죠! 처음 보는 단어일지라도 문자(알파벳)들의 조합으로 나타내어 OOV 문제를 해결할 수 있다는 장점\n",
    "-  position encoding(포지션 인코딩)도 함께 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-accounting",
   "metadata": {},
   "source": [
    "##### 2) Masked Multi-Head Attention\n",
    "-  모든 것을 병렬적으로 처리하는 트랜스포머에게 자기회귀적(Autoregressive)인 특성을 부여하기 위해 만든 장치\n",
    "   - 자기회귀적 : 훈련 단계에서 디코더에게 정답 문장을 매 스텝 단위로 단어 하나씩 알려주고 그다음 단어를 예측(Next Token Prediction)하게 하는 형태로 학습되는 형태\n",
    "   \n",
    "- 두 개의 Objective가 존재, 한 모델에서 동시에 output을 낸다. (두 가지의 문제를 동시에 푼다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "negative-memorial",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 41)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m41\u001b[0m\n\u001b[0;31m    w = tf.matmul(q, k, transpose_b=True)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "#  GPT의 모델 부분 코드\n",
    "\n",
    "class TFAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, nx, n_ctx, config, scale=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        n_state = nx  # in Attention: n_state=768 (nx=n_embd)\n",
    "        # [switch nx => n_state from Block to Attention to keep identical to TF implem]\n",
    "        assert n_state % config.n_head == 0\n",
    "        self.n_ctx = n_ctx\n",
    "        self.n_head = config.n_head\n",
    "        self.split_size = n_state\n",
    "        self.scale = scale\n",
    "        self.output_attentions = config.output_attentions\n",
    "\n",
    "        self.c_attn = TFConv1D(n_state * 3, nx, initializer_range=config.initializer_range, name=\"c_attn\")\n",
    "        self.c_proj = TFConv1D(n_state, nx, initializer_range=config.initializer_range, name=\"c_proj\")\n",
    "        self.attn_dropout = tf.keras.layers.Dropout(config.attn_pdrop)\n",
    "        self.resid_dropout = tf.keras.layers.Dropout(config.resid_pdrop)\n",
    "        self.pruned_heads = set()\n",
    "\n",
    "    def prune_heads(self, heads):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def causal_attention_mask(nd, ns, dtype):\n",
    "        \"\"\"\n",
    "        1-2) masked attention에서 설명한 masking 부분\n",
    "        \"\"\"\n",
    "        i = tf.range(nd)[:, None]\n",
    "        j = tf.range(ns)\n",
    "        m = i >= j - ns + nd\n",
    "        return tf.cast(m, dtype)\n",
    "\n",
    "    def _attn(self, q, k, v, attention_mask, head_mask, output_attentions, training=False):\n",
    "\t\t\t\t\"\"\"\n",
    "\t\t\t\t1-2) attention 계산\n",
    "        q, k, v 의 shape : [batch, heads, sequence, features]\n",
    "\t\t\t\t\"\"\"\n",
    "\n",
    "        w = tf.matmul(q, k, transpose_b=True)\n",
    "        if self.scale:\n",
    "            dk = tf.cast(shape_list(k)[-1], tf.float32)  # scale attention_scores\n",
    "            w = w / tf.math.sqrt(dk)\n",
    "\n",
    "        # w shape : [batch, heads, dst_sequence, src_sequence]\n",
    "        _, _, nd, ns = shape_list(w)\n",
    "        b = self.causal_attention_mask(nd, ns, dtype=w.dtype)\n",
    "        b = tf.reshape(b, [1, 1, nd, ns])\n",
    "        w = w * b - 1e4 * (1 - b)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # attention mask 적용\n",
    "            w = w + attention_mask\n",
    "\n",
    "        w = tf.nn.softmax(w, axis=-1)\n",
    "        w = self.attn_dropout(w, training=training)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            w = w * head_mask\n",
    "\n",
    "        outputs = [tf.matmul(w, v)]\n",
    "        if output_attentions:\n",
    "            outputs.append(w)\n",
    "        return outputs\n",
    "\n",
    "    def merge_heads(self, x):\n",
    "        x = tf.transpose(x, [0, 2, 1, 3])\n",
    "        x_shape = shape_list(x)\n",
    "        new_x_shape = x_shape[:-2] + [x_shape[-2] * x_shape[-1]]\n",
    "        return tf.reshape(x, new_x_shape)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        x_shape = shape_list(x)\n",
    "        new_x_shape = x_shape[:-1] + [self.n_head, x_shape[-1] // self.n_head]\n",
    "        x = tf.reshape(x, new_x_shape)\n",
    "        return tf.transpose(x, (0, 2, 1, 3))  # (batch, head, seq_length, head_features)\n",
    "\n",
    "    def call(self, x, layer_past, attention_mask, head_mask, use_cache, output_attentions, training=False):\n",
    "        x = self.c_attn(x)\n",
    "        query, key, value = tf.split(x, 3, axis=2)\n",
    "        query = self.split_heads(query)\n",
    "        key = self.split_heads(key)\n",
    "        value = self.split_heads(value)\n",
    "        if layer_past is not None:\n",
    "            past_key, past_value = tf.unstack(layer_past, axis=0)\n",
    "            key = tf.concat([past_key, key], axis=-2)\n",
    "            value = tf.concat([past_value, value], axis=-2)\n",
    "\n",
    "        # keras serialization을 위한 코드\n",
    "        if use_cache:\n",
    "            present = tf.stack([key, value], axis=0)\n",
    "        else:\n",
    "            present = (None,)\n",
    "\n",
    "        attn_outputs = self._attn(query, key, value, attention_mask, head_mask, output_attentions, training=training)\n",
    "        a = attn_outputs[0]\n",
    "\n",
    "        a = self.merge_heads(a)\n",
    "        a = self.c_proj(a)\n",
    "        a = self.resid_dropout(a, training=training)\n",
    "\n",
    "        outputs = [a, present] + attn_outputs[1:]\n",
    "        return outputs  # a, present, (attentions)\n",
    "\n",
    "\n",
    "class TFMLP(tf.keras.layers.Layer):\n",
    "\"\"\"\n",
    "Transformer의 Decoder Block에서 Feed Foward를 구현해 둔 부분\n",
    "\"\"\"\n",
    "    def __init__(self, n_state, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        nx = config.n_embd\n",
    "        self.c_fc = TFConv1D(n_state, nx, initializer_range=config.initializer_range, name=\"c_fc\")\n",
    "        self.c_proj = TFConv1D(nx, n_state, initializer_range=config.initializer_range, name=\"c_proj\")\n",
    "        self.act = get_tf_activation(\"gelu\")\n",
    "        self.dropout = tf.keras.layers.Dropout(config.resid_pdrop)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        h = self.act(self.c_fc(x)) # conv1d로 flatten 후 activation 적용\n",
    "        h2 = self.c_proj(h)\n",
    "        h2 = self.dropout(h2, training=training)\n",
    "        return h2\n",
    "\n",
    "\n",
    "class TFBlock(tf.keras.layers.Layer):\n",
    "\"\"\"\n",
    "Transformer의 Decoder Block을 구현해 둔 부분\n",
    "\"\"\"\n",
    "    def __init__(self, n_ctx, config, scale=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        nx = config.n_embd\n",
    "        inner_dim = config.n_inner if config.n_inner is not None else 4 * nx\n",
    "        self.ln_1 = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name=\"ln_1\")\n",
    "        self.attn = TFAttention(nx, n_ctx, config, scale, name=\"attn\")\n",
    "        self.ln_2 = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name=\"ln_2\")\n",
    "        self.mlp = TFMLP(inner_dim, config, name=\"mlp\")\n",
    "\n",
    "    def call(self, x, layer_past, attention_mask, head_mask, use_cache, output_attentions, training=False):\n",
    "        a = self.ln_1(x)\n",
    "        output_attn = self.attn(\n",
    "            a, layer_past, attention_mask, head_mask, use_cache, output_attentions, training=training\n",
    "        )\n",
    "        a = output_attn[0]  # output_attn: a, present, (attentions)\n",
    "        x = x + a\n",
    "\n",
    "        m = self.ln_2(x)\n",
    "        m = self.mlp(m, training=training)\n",
    "        x = x + m\n",
    "\n",
    "        outputs = [x] + output_attn[1:]\n",
    "        return outputs  # x, present, (attentions)\n",
    "\n",
    "\n",
    "@keras_serializable\n",
    "class TFGPT2MainLayer(tf.keras.layers.Layer):\n",
    "\"\"\"\n",
    "모델의 전체 구조\n",
    "\"\"\"\n",
    "    config_class = GPT2Config\n",
    "\n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super().__init__(*inputs, **kwargs)\n",
    "        self.output_attentions = config.output_attentions\n",
    "        self.output_hidden_states = config.output_hidden_states\n",
    "        self.use_cache = config.use_cache\n",
    "        self.return_dict = config.use_return_dict\n",
    "\n",
    "        self.num_hidden_layers = config.n_layer\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "        self.wte = TFSharedEmbeddings(\n",
    "            config.vocab_size, config.hidden_size, initializer_range=config.initializer_range, name=\"wte\"\n",
    "        )\n",
    "        self.wpe = tf.keras.layers.Embedding(\n",
    "            config.n_positions,\n",
    "            config.n_embd,\n",
    "            embeddings_initializer=get_initializer(config.initializer_range),\n",
    "            name=\"wpe\",\n",
    "        )\n",
    "        self.drop = tf.keras.layers.Dropout(config.embd_pdrop)\n",
    "        self.h = [TFBlock(config.n_ctx, config, scale=True, name=\"h_._{}\".format(i)) for i in range(config.n_layer)]\n",
    "        self.ln_f = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name=\"ln_f\")\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.wte\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.wte.weight = value\n",
    "        self.wte.vocab_size = self.wte.weight.shape[0]\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        \"\"\"\n",
    "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        inputs,\n",
    "        past=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        training=False,\n",
    "    ):\n",
    "        if isinstance(inputs, (tuple, list)):\n",
    "            input_ids = inputs[0]\n",
    "            past = inputs[1] if len(inputs) > 1 else past\n",
    "            attention_mask = inputs[2] if len(inputs) > 2 else attention_mask\n",
    "            token_type_ids = inputs[3] if len(inputs) > 3 else token_type_ids\n",
    "            position_ids = inputs[4] if len(inputs) > 4 else position_ids\n",
    "            head_mask = inputs[5] if len(inputs) > 5 else head_mask\n",
    "            inputs_embeds = inputs[6] if len(inputs) > 6 else inputs_embeds\n",
    "            use_cache = inputs[7] if len(inputs) > 7 else use_cache\n",
    "            output_attentions = inputs[8] if len(inputs) > 8 else output_attentions\n",
    "            output_hidden_states = inputs[9] if len(inputs) > 9 else output_hidden_states\n",
    "            return_dict = inputs[10] if len(inputs) > 10 else return_dict\n",
    "            assert len(inputs) <= 11, \"Too many inputs.\"\n",
    "        elif isinstance(inputs, (dict, BatchEncoding)):\n",
    "            input_ids = inputs.get(\"input_ids\")\n",
    "            past = inputs.get(\"past\", past)\n",
    "            attention_mask = inputs.get(\"attention_mask\", attention_mask)\n",
    "            token_type_ids = inputs.get(\"token_type_ids\", token_type_ids)\n",
    "            position_ids = inputs.get(\"position_ids\", position_ids)\n",
    "            head_mask = inputs.get(\"head_mask\", head_mask)\n",
    "            inputs_embeds = inputs.get(\"inputs_embeds\", inputs_embeds)\n",
    "            use_cache = inputs.get(\"use_cache\", use_cache)\n",
    "            output_attentions = inputs.get(\"output_attentions\", output_attentions)\n",
    "            output_hidden_states = inputs.get(\"output_hidden_states\", output_hidden_states)\n",
    "            return_dict = inputs.get(\"return_dict\", return_dict)\n",
    "            assert len(inputs) <= 11, \"Too many inputs.\"\n",
    "        else:\n",
    "            input_ids = inputs\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.output_attentions\n",
    "        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.output_hidden_states\n",
    "        use_cache = use_cache if use_cache is not None else self.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.return_dict\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = shape_list(input_ids)\n",
    "            input_ids = tf.reshape(input_ids, [-1, input_shape[-1]])\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = shape_list(inputs_embeds)[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        if past is None:\n",
    "            past_length = 0\n",
    "            past = [None] * len(self.h)\n",
    "        else:\n",
    "            past_length = shape_list(past[0][0])[-2]\n",
    "        if position_ids is None:\n",
    "            position_ids = tf.range(past_length, input_shape[-1] + past_length, dtype=tf.int32)[tf.newaxis, :]\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # 3D attention mask 만들기\n",
    "            # Sizes : [batch_size, 1, 1, to_seq_length]\n",
    "            # 3D attention mask를 [batch_size, num_heads, from_seq_length, to_seq_length]로 브로드캐스팅\n",
    "\n",
    "            attention_mask = attention_mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "            # attention_mask가 1.0이면 포함, 0.0이면 미포함 하여 attention 계산\n",
    "            attention_mask = tf.cast(attention_mask, tf.float32)\n",
    "            attention_mask = (1.0 - attention_mask) * -10000.0\n",
    "        else:\n",
    "            attention_mask = None\n",
    "\n",
    "        # head_mask가 1.0이면, head를 유지\n",
    "        # attention_probs : shape bsz x n_heads x N x N\n",
    "        # input head_mask : shape [num_heads] 또는 [num_hidden_layers x num_heads]\n",
    "        # head_mask : shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        if head_mask is not None:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            head_mask = [None] * self.num_hidden_layers\n",
    "            # head_mask = tf.constant([0] * self.num_hidden_layers)\n",
    "\n",
    "        position_ids = tf.reshape(position_ids, [-1, shape_list(position_ids)[-1]])\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.wte(input_ids, mode=\"embedding\")\n",
    "        position_embeds = self.wpe(position_ids)\n",
    "        if token_type_ids is not None:\n",
    "            token_type_ids = tf.reshape(token_type_ids, [-1, shape_list(token_type_ids)[-1]])\n",
    "            token_type_embeds = self.wte(token_type_ids, mode=\"embedding\")\n",
    "        else:\n",
    "            token_type_embeds = 0\n",
    "        hidden_states = inputs_embeds + position_embeds + token_type_embeds\n",
    "        hidden_states = self.drop(hidden_states, training=training)\n",
    "\n",
    "        output_shape = input_shape + [shape_list(hidden_states)[-1]]\n",
    "\n",
    "        presents = () if use_cache else None\n",
    "        all_attentions = () if output_attentions else None\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        for i, (block, layer_past) in enumerate(zip(self.h, past)):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (tf.reshape(hidden_states, output_shape),)\n",
    "\n",
    "            outputs = block(\n",
    "                hidden_states,\n",
    "                layer_past,\n",
    "                attention_mask,\n",
    "                head_mask[i],\n",
    "                use_cache,\n",
    "                output_attentions,\n",
    "                training=training,\n",
    "            )\n",
    "\n",
    "            hidden_states, present = outputs[:2]\n",
    "            if use_cache:\n",
    "                presents = presents + (present,)\n",
    "\n",
    "            if output_attentions:\n",
    "                all_attentions = all_attentions + (outputs[2],)\n",
    "\n",
    "        hidden_states = self.ln_f(hidden_states)\n",
    "\n",
    "        hidden_states = tf.reshape(hidden_states, output_shape)\n",
    "        # Add last hidden state\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if output_attentions:\n",
    "            attention_output_shape = input_shape[:-1] + [-1] + shape_list(all_attentions[0])[-2:]\n",
    "            all_attentions = tuple(tf.reshape(t, attention_output_shape) for t in all_attentions)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, presents, all_hidden_states, all_attentions] if v is not None)\n",
    "\n",
    "        return TFBaseModelOutputWithPast(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=presents,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-mouse",
   "metadata": {},
   "source": [
    "<B>input을 변형시켜  classification, entailment 등등 다양한 문제를 풀 수 있다.</B>\n",
    "- 예를 들어 classification task를 풀기 위해 finetuning을 하게 된다면 $<start> <input text> <extract> <class>$ 이렇게 구성된 데이터셋을 학습시키면 되는 것이죠. GPT는 이 데이터셋에 맞추어서 weight들을 조정하게 될 것입니다.\n",
    "\n",
    "finetuning이 끝나고 테스트 시에는 $<start> <input text> <extract>$을 input으로 주면 해당 시퀀스에 뒤이어 나올 토큰 즉 <class>를 생성하게 되는 것이죠."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-jersey",
   "metadata": {},
   "source": [
    "### GPT vs. GPT 논문\n",
    "\n",
    "- <a href=\"https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf\" target=\"_blank\"> GPT 논문 </a>\n",
    "- <a href=\"https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\" target=\"_blank\">  GPT-2 논문 </a>\n",
    "\n",
    "- <a href=\"https://demo.allennlp.org/next-token-lm?text=AllenNLP%20is\" target=\"_blank\">  AllenNLP - Demo </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-butter",
   "metadata": {},
   "source": [
    "## 13-5 BERT\n",
    "- <a href=\"https://arxiv.org/abs/1810.04805\" target=\"_blank\"> BERT 논문 보기 </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "toxic-village",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-84d77f9d14eb>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-84d77f9d14eb>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    [BERTvsGPT구조]  GPT는 input을 한 방향 / BERT는 양방향(bi-direction)으로 input, 트랜스포머의 encoder만을 사용한 모델\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "[BERTvsGPT구조]  GPT는 input을 한 방향 / BERT는 양방향(bi-direction)으로 input, 트랜스포머의 encoder만을 사용한 모델\n",
    "![BERTvsGPT구조](https://user-images.githubusercontent.com/70866993/139850839-af4682c2-d6f0-475a-933b-eb51cad39fe6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-stress",
   "metadata": {},
   "source": [
    "#### 1) 다른 임베딩 체계 3가지\n",
    "- 1)Token Embeddiong : 우선 wordpiece를 이용하여 3만 개의 vocabulary를 학습한다. 학습한 wordpiece model을 이용하여 toten 들을 임베딩해준다.\n",
    "- 2)segment Embeddiong : BERT 의 두가지 sentence(논문상 텍스트 덩어리 의미)를 입력 받기 때문에 이 두가지 sentence를 구분할 필요가 있다. segment embedding은 바로 이를 위해서 존재, 텍스트들 덩어리를 나누어주는 역할을 한다. \n",
    "- 3) Position Embedding : 문장 내에 절대적인 위치(순서)를 알려주기 위해 필요한 것이 position embedding, 학습을 통해 position 습득"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-accent",
   "metadata": {},
   "source": [
    "<mark> position encoding과 position embedding은 다른 개념이다. </mark>\n",
    "\n",
    "- encoding은 one-hot-encoding(원핫 인코딩)처럼 미리 정해진 값을 주는 것이지만 embedding은 그 값이 정해진 것이 아니라 학습을 통해 습득하는 것을 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-trader",
   "metadata": {},
   "source": [
    "#### 2)토큰 종류 \n",
    "- [CLS] : sentence의 시작을 알리는 토큰\n",
    "\n",
    "- [SEP] : sentence의 종결을 알리는 토큰, sentence를 구분하는 데에 사용하기도 함\n",
    "\n",
    "- [MASK]: 마스크 토큰\n",
    "\n",
    "- [PAD] : 배치 데이터의 길이를 맞춰주기 위한 토큰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-instruction",
   "metadata": {},
   "source": [
    "#### 3) 활성화 함수(Activation Function) : GELU\n",
    "- ReLU와는 달리 GELU는 음수에서도 완만한 곡선을 그리며 미분을 가능, 성능이 더욱 좋아진다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-marriage",
   "metadata": {},
   "source": [
    "####  4) 양방향 학습 강조모델-/,  MLM과 NSP는 따로 학습되는 것이 아니라 동시에 이뤄집니다.\n",
    "- 1) Masked LM(MLM) :  마스크 된 토큰([MASK)만 맞추면 되는 masked LM(MLM)을 제안\n",
    "    - BERT는 학습 데이터의 전체에서  80%는 [MASK]토큰, 10%는 무작위로 랜덤한 토큰으로 변경, 10% 원래의 토큰 그대로 사용\n",
    "    - MASK 토큰이 아닌 것들도 예측을 하도록 학습하여 문장 자체에 대한 전반적인 이해(문맥에 대한 이해)를 할 수 있도록 해주는 겁니다.\n",
    "    \n",
    "- 2) Next Sentence Prediction (NSP) :  마스크 된 토큰을 맞추는 것과 동시에 또 다른 task를 함께 학습\n",
    "    - task가 너무 쉬워지는 것을 방지하기 위해 max_num_tokens라는 것을 정의한다.\n",
    "        - 데이터의 90%는 max_num_tokens = max_sequence_length가 같도록 만들기\n",
    "        - 데이터의 10%는 max_num_tokens < max_sequence_length보다 짧게 되도록 랜덤 정하기\n",
    "        - 두 개의 sentence의 단어 총수가 max_num_tokens보다 작아질 때까지 두 sentence 중 단어 수가 많은 쪽의 문장 맨 앞 또는 맨 뒤 단어를 하나씩 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-dependence",
   "metadata": {},
   "source": [
    "#### 5) -  Fine-tuning Task\n",
    "- BERT 또한 한 모델이 다양한 task들을 수행하기 때문에 input transformation을 이용\n",
    "     - classification 같은 경우는 [CLS] 시작을 알리는 토큰을, QA와 같이 문장이나 단어들이 나와야 하는 경우에는 토큰들의 벡터를 output layer에 넣어 output을 산출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-monitoring",
   "metadata": {},
   "source": [
    "#### 6) 모델 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "generic-microwave",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-4-480160e0c3a8>, line 252)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-480160e0c3a8>\"\u001b[0;36m, line \u001b[0;32m252\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m       \n^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class TFBertPreTrainingLoss:\n",
    "    \"\"\"\n",
    "    BERT의 경우 Pretraining으로 NSP + MLM 두 가지를 함께 학습하게 됨. 그것을 위한 loss \n",
    "\t\t-100으로 label(logit)이 되어있는 경우 loss 계산 시 제외\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_loss(self, labels, logits):\n",
    "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction=tf.keras.losses.Reduction.NONE\n",
    "        )\n",
    "\n",
    "        masked_lm_active_loss = tf.not_equal(tf.reshape(labels[\"labels\"], (-1,)), -100)\n",
    "        masked_lm_reduced_logits = tf.boolean_mask(\n",
    "            tf.reshape(logits[0], (-1, shape_list(logits[0])[2])),\n",
    "            masked_lm_active_loss,\n",
    "        )\n",
    "        masked_lm_labels = tf.boolean_mask(tf.reshape(labels[\"labels\"], (-1,)), masked_lm_active_loss)\n",
    "        next_sentence_active_loss = tf.not_equal(tf.reshape(labels[\"next_sentence_label\"], (-1,)), -100)\n",
    "        next_sentence_reduced_logits = tf.boolean_mask(tf.reshape(logits[1], (-1, 2)), next_sentence_active_loss)\n",
    "        next_sentence_label = tf.boolean_mask(\n",
    "            tf.reshape(labels[\"next_sentence_label\"], (-1,)), mask=next_sentence_active_loss\n",
    "        )\n",
    "        masked_lm_loss = loss_fn(masked_lm_labels, masked_lm_reduced_logits)\n",
    "        next_sentence_loss = loss_fn(next_sentence_label, next_sentence_reduced_logits)\n",
    "        masked_lm_loss = tf.reshape(masked_lm_loss, (-1, shape_list(next_sentence_loss)[0]))\n",
    "        masked_lm_loss = tf.reduce_mean(masked_lm_loss, 0)\n",
    "\n",
    "        return masked_lm_loss + next_sentence_loss\n",
    "\n",
    "\n",
    "class TFBertEmbeddings(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "\t\t1-1)에 해당하는 부분으로 3가지 embedding을 만들고 그 embedding을 모두 합산하여 layer normalize와 dropout을 적용\n",
    "\t\t\"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.initializer_range = config.initializer_range\n",
    "        self.position_embeddings = tf.keras.layers.Embedding(\n",
    "            config.max_position_embeddings,\n",
    "            config.hidden_size,\n",
    "            embeddings_initializer=get_initializer(self.initializer_range),\n",
    "            name=\"position_embeddings\",\n",
    "        )\n",
    "        self.token_type_embeddings = tf.keras.layers.Embedding(\n",
    "            config.type_vocab_size,\n",
    "            config.hidden_size,\n",
    "            embeddings_initializer=get_initializer(self.initializer_range),\n",
    "            name=\"token_type_embeddings\",\n",
    "        )\n",
    "\n",
    "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n",
    "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"shared word embedding layer \"\"\"\n",
    "        with tf.name_scope(\"word_embeddings\"):\n",
    "            # Create and initialize weights. The random normal initializer was chosen\n",
    "            # arbitrarily, and works well.\n",
    "            self.word_embeddings = self.add_weight(\n",
    "                \"weight\",\n",
    "                shape=[self.vocab_size, self.hidden_size],\n",
    "                initializer=get_initializer(self.initializer_range),\n",
    "            )\n",
    "\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        position_ids=None,\n",
    "        token_type_ids=None,\n",
    "        inputs_embeds=None,\n",
    "        mode=\"embedding\",\n",
    "        training=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        input의 token embeddings\n",
    "        Args:\n",
    "            inputs: int64 tensors (shape [batch_size, length]) 3개를 담은 리스트: (input_ids, position_ids, token_type_ids)\n",
    "            mode: \"embedding\" | \"linear\"\n",
    "        Returns:\n",
    "            outputs: mode == \"embedding\"; output embedding tensor(float32, shape [batch_size, length, embedding_size])\n",
    "\t\t\t\t\t\t\t\t\t\t mode == \"linear\", output linear tensor(float32, shape [batch_size, length, vocab_size])\n",
    "        Raises:\n",
    "            ValueError: if mode is not valid.\n",
    "\t\t\t\t\"\"\"\n",
    "\n",
    "        if mode == \"embedding\":\n",
    "            return self._embedding(input_ids, position_ids, token_type_ids, inputs_embeds, training=training)\n",
    "        elif mode == \"linear\":\n",
    "            return self._linear(input_ids)\n",
    "        else:\n",
    "            raise ValueError(\"mode {} is not valid.\".format(mode))\n",
    "\n",
    "    def _embedding(self, input_ids, position_ids, token_type_ids, inputs_embeds, training=False):\n",
    "        \"\"\"input tensor에 기반하여 임베딩 적용\"\"\"\n",
    "        assert not (input_ids is None and inputs_embeds is None)\n",
    "\n",
    "        if input_ids is not None:\n",
    "            input_shape = shape_list(input_ids)\n",
    "        else:\n",
    "            input_shape = shape_list(inputs_embeds)[:-1]\n",
    "\n",
    "        seq_length = input_shape[1]\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = tf.range(seq_length, dtype=tf.int32)[tf.newaxis, :]\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = tf.fill(input_shape, 0)\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = tf.gather(self.word_embeddings, input_ids)\n",
    "\n",
    "        position_embeddings = tf.cast(self.position_embeddings(position_ids), inputs_embeds.dtype)\n",
    "        token_type_embeddings = tf.cast(self.token_type_embeddings(token_type_ids), inputs_embeds.dtype)\n",
    "        embeddings = inputs_embeds + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings, training=training)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def _linear(self, inputs):\n",
    "        \"\"\"\n",
    " \t\t\t  linear layer를 통해서 input의 logit을 계산\n",
    "        Args:\n",
    "            inputs: float32 tensor (shape [batch_size, length, hidden_size])\n",
    "        Returns:\n",
    "            float32 tensor (shape [batch_size, length, vocab_size])\n",
    "        \"\"\"\n",
    "        batch_size = shape_list(inputs)[0]\n",
    "        length = shape_list(inputs)[1]\n",
    "        x = tf.reshape(inputs, [-1, self.hidden_size])\n",
    "        logits = tf.matmul(x, self.word_embeddings, transpose_b=True)\n",
    "\n",
    "        return tf.reshape(logits, [batch_size, length, self.vocab_size])\n",
    "\n",
    "\n",
    "class TFBertSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        if config.hidden_size % config.num_attention_heads != 0:\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n",
    "            )\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        assert config.hidden_size % config.num_attention_heads == 0\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        self.query = tf.keras.layers.Dense(\n",
    "            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"query\"\n",
    "        )\n",
    "        self.key = tf.keras.layers.Dense(\n",
    "            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"key\"\n",
    "        )\n",
    "        self.value = tf.keras.layers.Dense(\n",
    "            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"value\"\n",
    "        )\n",
    "        self.dropout = tf.keras.layers.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_attention_heads, self.attention_head_size))\n",
    "\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, hidden_states, attention_mask, head_mask, output_attentions, training=False):\n",
    "        batch_size = shape_list(hidden_states)[0]\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer, batch_size)\n",
    "\n",
    "        # \"query\"와 \"key\"의 dot product : raw attention scores\n",
    "        attention_scores = tf.matmul(\n",
    "            query_layer, key_layer, transpose_b=True\n",
    "        )  # (batch size, num_heads, seq_len_q, seq_len_k)\n",
    "        dk = tf.cast(shape_list(key_layer)[-1], attention_scores.dtype)  # scale attention_scores\n",
    "        attention_scores = attention_scores / tf.math.sqrt(dk)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = tf.nn.softmax(attention_scores, axis=-1)\n",
    "\n",
    "        attention_probs = self.dropout(attention_probs, training=training)\n",
    "\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        context_layer = tf.matmul(attention_probs, value_layer)\n",
    "        context_layer = tf.transpose(context_layer, perm=[0, 2, 1, 3])\n",
    "        context_layer = tf.reshape(\n",
    "            context_layer, (batch_size, -1, self.all_head_size)\n",
    "        )  # (batch_size, seq_len_q, all_head_size)\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class TFBertSelfOutput(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n",
    "        )\n",
    "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n",
    "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def call(self, hidden_states, input_tensor, training=False):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states, training=training)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class TFBertAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.self_attention = TFBertSelfAttention(config, name=\"self\")\n",
    "        self.dense_output = TFBertSelfOutput(config, name=\"output\")\n",
    "\n",
    "    def prune_heads(self, heads):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def call(self, input_tensor, attention_mask, head_mask, output_attentions, training=False):\n",
    "        self_outputs = self.self_attention(\n",
    "            input_tensor, attention_mask, head_mask, output_attentions, training=training\n",
    "        )\n",
    "        attention_output = self.dense_output(self_outputs[0], input_tensor, training=training)\n",
    "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class TFBertIntermediate(tf.keras.layers.Layer):\n",
    "\"\"\"\n",
    "Transformer Block에서의 feedforward\n",
    "\"\"\"\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            config.intermediate_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n",
    "        )\n",
    "\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.intermediate_act_fn = get_tf_activation(config.hidden_act)\n",
    "        else:\n",
    "            self.intermediate_act_fn = config.hidden_act\n",
    "\n",
    "    def call(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class TFBertOutput(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n",
    "        )\n",
    "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n",
    "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def call(self, hidden_states, input_tensor, training=False):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states, training=training)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class TFBertLayer(tf.keras.layers.Layer):\n",
    "\"\"\"\n",
    "Transformer Encoder Block과 동일한 구조 : Attention,Feedforward,dropout,layer nomalization\n",
    "\"\"\"\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.attention = TFBertAttention(config, name=\"attention\")\n",
    "        self.intermediate = TFBertIntermediate(config, name=\"intermediate\")\n",
    "        self.bert_output = TFBertOutput(config, name=\"output\")\n",
    "\n",
    "    def call(self, hidden_states, attention_mask, head_mask, output_attentions, training=False):\n",
    "        attention_outputs = self.attention(\n",
    "            hidden_states, attention_mask, head_mask, output_attentions, training=training\n",
    "        )\n",
    "        attention_output = attention_outputs[0]\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.bert_output(intermediate_output, attention_output, training=training)\n",
    "        outputs = (layer_output,) + attention_outputs[1:]  # add attentions if we output them\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class TFBertEncoder(tf.keras.layers.Layer):\n",
    "\"\"\"\n",
    "Transformer Encoder Block(코드 상에서 TFBertLayer)를 n_layer만큼 여러개 쌓은 구조\n",
    "\"\"\"\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.layer = [TFBertLayer(config, name=\"layer_._{}\".format(i)) for i in range(config.num_hidden_layers)]\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask,\n",
    "        head_mask,\n",
    "        output_attentions,\n",
    "        output_hidden_states,\n",
    "        return_dict,\n",
    "        training=False,\n",
    "    ):\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_attentions = () if output_attentions else None\n",
    "\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_outputs = layer_module(\n",
    "                hidden_states, attention_mask, head_mask[i], output_attentions, training=training\n",
    "            )\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_attentions = all_attentions + (layer_outputs[1],)\n",
    "\n",
    "        # Add last layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, all_hidden_states, all_attentions] if v is not None)\n",
    "\n",
    "        return TFBaseModelOutput(\n",
    "            last_hidden_state=hidden_states, hidden_states=all_hidden_states, attentions=all_attentions\n",
    "        )\n",
    "\n",
    "\n",
    "class TFBertPooler(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            config.hidden_size,\n",
    "            kernel_initializer=get_initializer(config.initializer_range),\n",
    "            activation=\"tanh\",\n",
    "            name=\"dense\",\n",
    "        )\n",
    "\n",
    "    def call(self, hidden_states):\n",
    "        # 첫 번째 토큰의 hidden state를 얻기 위해 pool\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "\n",
    "        return pooled_output\n",
    "\n",
    "\n",
    "class TFBertPredictionHeadTransform(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n",
    "        )\n",
    "\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.transform_act_fn = get_tf_activation(config.hidden_act)\n",
    "        else:\n",
    "            self.transform_act_fn = config.hidden_act\n",
    "\n",
    "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n",
    "\n",
    "    def call(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.transform_act_fn(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class TFBertLMPredictionHead(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, input_embeddings, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.transform = TFBertPredictionHeadTransform(config, name=\"transform\")\n",
    "\n",
    "        # input embeddings과 동일한 weight를 가지고 있지만 각각의 token에 대하여 output만 바이어스를 가지고 있음\n",
    "        self.input_embeddings = input_embeddings\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bias = self.add_weight(shape=(self.vocab_size,), initializer=\"zeros\", trainable=True, name=\"bias\")\n",
    "\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, hidden_states):\n",
    "        hidden_states = self.transform(hidden_states)\n",
    "        hidden_states = self.input_embeddings(hidden_states, mode=\"linear\")\n",
    "        hidden_states = hidden_states + self.bias\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class TFBertMLMHead(tf.keras.layers.Layer):\n",
    "\"\"\"\n",
    "2-1)Masked LM을 위한 class\n",
    "\"\"\"\n",
    "    def __init__(self, config, input_embeddings, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.predictions = TFBertLMPredictionHead(config, input_embeddings, name=\"predictions\")\n",
    "\n",
    "    def call(self, sequence_output):\n",
    "        prediction_scores = self.predictions(sequence_output)\n",
    "\n",
    "        return prediction_scores\n",
    "\n",
    "\n",
    "class TFBertNSPHead(tf.keras.layers.Layer):\n",
    "\"\"\"\n",
    "2-2)NSP를 위한 class\n",
    "\"\"\"\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.seq_relationship = tf.keras.layers.Dense(\n",
    "            2, kernel_initializer=get_initializer(config.initializer_range), name=\"seq_relationship\"\n",
    "        )\n",
    "\n",
    "    def call(self, pooled_output):\n",
    "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
    "\n",
    "        return seq_relationship_score\n",
    "\n",
    "\n",
    "@keras_serializable\n",
    "class TFBertMainLayer(tf.keras.layers.Layer):\n",
    "\"\"\"\n",
    "모델의 전체 구조\n",
    "\"\"\"\n",
    "    config_class = BertConfig\n",
    "\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.num_hidden_layers = config.num_hidden_layers\n",
    "        self.initializer_range = config.initializer_range\n",
    "        self.output_attentions = config.output_attentions\n",
    "        self.output_hidden_states = config.output_hidden_states\n",
    "        self.return_dict = config.use_return_dict\n",
    "        self.embeddings = TFBertEmbeddings(config, name=\"embeddings\")\n",
    "        self.encoder = TFBertEncoder(config, name=\"encoder\")\n",
    "        self.pooler = TFBertPooler(config, name=\"pooler\")\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embeddings\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embeddings.word_embeddings = value\n",
    "        self.embeddings.vocab_size = value.shape[0]\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        \"\"\"\n",
    "        heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        inputs,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        training=False,\n",
    "    ):\n",
    "        if isinstance(inputs, (tuple, list)):\n",
    "            input_ids = inputs[0]\n",
    "            attention_mask = inputs[1] if len(inputs) > 1 else attention_mask\n",
    "            token_type_ids = inputs[2] if len(inputs) > 2 else token_type_ids\n",
    "            position_ids = inputs[3] if len(inputs) > 3 else position_ids\n",
    "            head_mask = inputs[4] if len(inputs) > 4 else head_mask\n",
    "            inputs_embeds = inputs[5] if len(inputs) > 5 else inputs_embeds\n",
    "            output_attentions = inputs[6] if len(inputs) > 6 else output_attentions\n",
    "            output_hidden_states = inputs[7] if len(inputs) > 7 else output_hidden_states\n",
    "            return_dict = inputs[8] if len(inputs) > 8 else return_dict\n",
    "            assert len(inputs) <= 9, \"Too many inputs.\"\n",
    "        elif isinstance(inputs, (dict, BatchEncoding)):\n",
    "            input_ids = inputs.get(\"input_ids\")\n",
    "            attention_mask = inputs.get(\"attention_mask\", attention_mask)\n",
    "            token_type_ids = inputs.get(\"token_type_ids\", token_type_ids)\n",
    "            position_ids = inputs.get(\"position_ids\", position_ids)\n",
    "            head_mask = inputs.get(\"head_mask\", head_mask)\n",
    "            inputs_embeds = inputs.get(\"inputs_embeds\", inputs_embeds)\n",
    "            output_attentions = inputs.get(\"output_attentions\", output_attentions)\n",
    "            output_hidden_states = inputs.get(\"output_hidden_states\", output_hidden_states)\n",
    "            return_dict = inputs.get(\"return_dict\", return_dict)\n",
    "            assert len(inputs) <= 9, \"Too many inputs.\"\n",
    "        else:\n",
    "            input_ids = inputs\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.output_attentions\n",
    "        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.output_hidden_states\n",
    "        return_dict = return_dict if return_dict is not None else self.return_dict\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = shape_list(input_ids)\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = shape_list(inputs_embeds)[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = tf.fill(input_shape, 1)\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = tf.fill(input_shape, 0)\n",
    "\n",
    "        embedding_output = self.embeddings(input_ids, position_ids, token_type_ids, inputs_embeds, training=training)\n",
    "\n",
    "\t\t\t\t# 3D attention mask 만들기\n",
    "        # Sizes : [batch_size, 1, 1, to_seq_length]\n",
    "        # 3D attention mask를 [batch_size, num_heads, from_seq_length, to_seq_length]로 브로드캐스팅\n",
    "\n",
    "        extended_attention_mask = attention_mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "\t\t\t\t# attention_mask가 1.0이면 포함, 0.0이면 미포함 하여 attention 계산\n",
    "        extended_attention_mask = tf.cast(extended_attention_mask, embedding_output.dtype)\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        # head_mask가 1.0이면, head를 유지\n",
    "        # attention_probs : shape bsz x n_heads x N x N\n",
    "        # input head_mask : shape [num_heads] 또는 [num_hidden_layers x num_heads]\n",
    "        # head_mask : shape [num_hidden_layers x batch x num_heads x seq_length x seq_length\n",
    "        if head_mask is not None:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            head_mask = [None] * self.num_hidden_layers\n",
    "            # head_mask = tf.constant([0] * self.num_hidden_layers)\n",
    "\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            extended_attention_mask,\n",
    "            head_mask,\n",
    "            output_attentions,\n",
    "            output_hidden_states,\n",
    "            return_dict,\n",
    "            training=training,\n",
    "        )\n",
    "\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "\n",
    "        if not return_dict:\n",
    "            return (\n",
    "                sequence_output,\n",
    "                pooled_output,\n",
    "            ) + encoder_outputs[1:]\n",
    "\n",
    "        return TFBaseModelOutputWithPooling(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooled_output,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-classroom",
   "metadata": {},
   "source": [
    "- TFBertEncoder 안에 반복적으로 사용되고 있는 TFBertLayer 레이어 구성을 자세히 살펴주세요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-florist",
   "metadata": {},
   "source": [
    "## transformer-XL\n",
    "- <a href=\"https://arxiv.org/pdf/1901.02860.pdf\" target=\"_blank\"> transformer-XL 논문보기 </a>\n",
    "-  기존의 언어 모델과 트랜스포머가 가지고 있던 한계점(context)를 반영하기 을 개선한 모델\n",
    "    - 비교적 짧은 문장에서의 context는 잘 학습했는데, sequence가 길어질수록 그 상관관계(long-term dependency)가 점점 떨어진다는 것이 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-organic",
   "metadata": {},
   "source": [
    "#### 1) transformer-XL 구조\n",
    "-  1) 기존 문제 \n",
    "        - 트랜스포머는 max_seq_length가 정해져 있습니다. 이전 segment에서 학습했던 context는 무시되고, 지금 학습을 하고 있는 segment 안에서만 context를 고려하게 됩니다. 즉 segment1과 segment2는 전혀 공유하는 context가 없이 단절되었다\n",
    "        - 슬라이딩을 하면서 생기는 문제 > 딱 한 개만큼만 슬라이딩하여 새로운 context를 만들고 다시 연산하여 하나의 단어를 예측 >연산에 드는 비용이 많이 든다. \n",
    "        -  Relative Positional Encodings  기존 트랜스포머의 포지션 인코딩은 각 segment 내에서의 절대적인 위치 정보를 인코딩하게 된다. \n",
    "        recurrence 메커니즘 사용시  segment들 사이에서의 상대적인 위치 정보가 필요\n",
    "-  2)  recurrence 메커니즘을 도입개선 : 학습 시에 이전 segment에서 계산했었던 hidden state를 사용, 를 통해 context fragmentation을 해결하고 long-term dependency를 유지\n",
    "    - 단, 이때 이전 segment들의 정보를 가진 hidden state들의 gradient는 더 이상 변하지 않도록 고정\n",
    "    - segment들 사이에서의 상대적인 위치 정보가 필요를 포지션 인코딩 방법으로 제안 ( 임베딩 레이어가 아닌 attention 연산 시에 주입)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-university",
   "metadata": {},
   "source": [
    "- 기존 트랜스포머는 query의 위치에 따라 그 query vector가 영향을 받았습니다. 그러나 절대적인 포지션이 아닌 <b>상대적인 포지션 정보</b>를 이용하게 되면서 query는 그 위치에 상관없이 똑같은 query vector를 사용하게 됩니다. 따라서 포지션에 상관없이 같은 값인 벡터 u와 v로 대신하게 되었습니다.\n",
    "\n",
    "- 이렇게 recurrence 메커니즘과 relative position encoding을 통해 transformer-XL은 auxiliary losses 없이도 뛰어난 성능을 낼 수 있었다고 말합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-powell",
   "metadata": {},
   "source": [
    "####  Transformer-XL의 모델 부분을 코드확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "muslim-destination",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 126)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m126\u001b[0m\n\u001b[0;31m    x_size = shape_list(x)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "class TFPositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, demb, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.inv_freq = 1 / (10000 ** (tf.range(0, demb, 2.0) / demb))\n",
    "\n",
    "    def call(self, pos_seq, bsz=None):\n",
    "        sinusoid_inp = tf.einsum(\"i,j->ij\", pos_seq, self.inv_freq)\n",
    "        pos_emb = tf.concat([tf.sin(sinusoid_inp), tf.cos(sinusoid_inp)], -1)\n",
    "\n",
    "        if bsz is not None:\n",
    "            return tf.tile(pos_emb[:, None, :], [1, bsz, 1])\n",
    "        else:\n",
    "            return pos_emb[:, None, :]\n",
    "\n",
    "\n",
    "class TFPositionwiseFF(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_inner, dropout, pre_lnorm=False, layer_norm_epsilon=1e-5, init_std=0.02, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_inner = d_inner\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.layer_1 = tf.keras.layers.Dense(\n",
    "            d_inner, kernel_initializer=get_initializer(init_std), activation=tf.nn.relu, name=\"CoreNet_._0\"\n",
    "        )\n",
    "        self.drop_1 = tf.keras.layers.Dropout(dropout)\n",
    "        self.layer_2 = tf.keras.layers.Dense(d_model, kernel_initializer=get_initializer(init_std), name=\"CoreNet_._3\")\n",
    "        self.drop_2 = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=layer_norm_epsilon, name=\"layer_norm\")\n",
    "\n",
    "        self.pre_lnorm = pre_lnorm\n",
    "\n",
    "    def call(self, inp, training=False):\n",
    "        if self.pre_lnorm:\n",
    "            # layer normalization + positionwise feed-forward\n",
    "            core_out = self.layer_norm(inp)\n",
    "            core_out = self.layer_1(core_out)\n",
    "            core_out = self.drop_1(core_out, training=training)\n",
    "            core_out = self.layer_2(core_out)\n",
    "            core_out = self.drop_2(core_out, training=training)\n",
    "\n",
    "            # residual connection\n",
    "            output = core_out + inp\n",
    "        else:\n",
    "            # positionwise feed-forward\n",
    "            core_out = self.layer_1(inp)\n",
    "            core_out = self.drop_1(core_out, training=training)\n",
    "            core_out = self.layer_2(core_out)\n",
    "            core_out = self.drop_2(core_out, training=training)\n",
    "\n",
    "            # residual connection + layer normalization\n",
    "            output = self.layer_norm(inp + core_out)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class TFRelPartialLearnableMultiHeadAttn(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_head,\n",
    "        d_model,\n",
    "        d_head,\n",
    "        dropout,\n",
    "        dropatt=0.0,\n",
    "        pre_lnorm=False,\n",
    "        r_r_bias=None,\n",
    "        r_w_bias=None,\n",
    "        layer_norm_epsilon=1e-5,\n",
    "        init_std=0.02,\n",
    "        output_attentions=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_model = d_model\n",
    "        self.d_head = d_head\n",
    "        self.dropout = dropout\n",
    "        self.output_attentions = output_attentions\n",
    "\n",
    "        self.qkv_net = tf.keras.layers.Dense(\n",
    "            3 * n_head * d_head, kernel_initializer=get_initializer(init_std), use_bias=False, name=\"qkv_net\"\n",
    "        )\n",
    "\n",
    "        self.drop = tf.keras.layers.Dropout(dropout)\n",
    "        self.dropatt = tf.keras.layers.Dropout(dropatt)\n",
    "        self.o_net = tf.keras.layers.Dense(\n",
    "            d_model, kernel_initializer=get_initializer(init_std), use_bias=False, name=\"o_net\"\n",
    "        )\n",
    "\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=layer_norm_epsilon, name=\"layer_norm\")\n",
    "\n",
    "        self.scale = 1 / (d_head ** 0.5)\n",
    "\n",
    "        self.pre_lnorm = pre_lnorm\n",
    "\n",
    "        if r_r_bias is not None and r_w_bias is not None:  # Biases are shared\n",
    "            self.r_r_bias = r_r_bias\n",
    "            self.r_w_bias = r_w_bias\n",
    "        else:\n",
    "            self.r_r_bias = None\n",
    "            self.r_w_bias = None\n",
    "\n",
    "        self.r_net = tf.keras.layers.Dense(\n",
    "            self.n_head * self.d_head, kernel_initializer=get_initializer(init_std), use_bias=False, name=\"r_net\"\n",
    "        )\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.r_r_bias is None or self.r_w_bias is None:  # Biases are not shared\n",
    "            self.r_r_bias = self.add_weight(\n",
    "                shape=(self.n_head, self.d_head), initializer=\"zeros\", trainable=True, name=\"r_r_bias\"\n",
    "            )\n",
    "            self.r_w_bias = self.add_weight(\n",
    "                shape=(self.n_head, self.d_head), initializer=\"zeros\", trainable=True, name=\"r_w_bias\"\n",
    "            )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def _rel_shift(self, x):\n",
    "\t\t\"\"\"\n",
    "\t\trelative attention을 수행하기 위한 masking\n",
    "\t\t\"\"\"\n",
    "\n",
    "        x_size = shape_list(x)\n",
    "\n",
    "        x = tf.pad(x, [[0, 0], [1, 0], [0, 0], [0, 0]])\n",
    "        x = tf.reshape(x, [x_size[1] + 1, x_size[0], x_size[2], x_size[3]])\n",
    "        x = tf.slice(x, [1, 0, 0, 0], [-1, -1, -1, -1])\n",
    "        x = tf.reshape(x, x_size)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def call(self, w, r, attn_mask, mems, head_mask, output_attentions, training=False):\n",
    "        \"\"\"\n",
    "\t\t\t\tw는 embedding, r은 postional embedding\n",
    "\t\t\t\t\"\"\"\n",
    "\n",
    "\t\t\t\tqlen, rlen, bsz = shape_list(w)[0], shape_list(r)[0], shape_list(w)[1]\n",
    "\n",
    "        if mems is not None:\n",
    "            cat = tf.concat([mems, w], 0)\n",
    "            if self.pre_lnorm:\n",
    "                w_heads = self.qkv_net(self.layer_norm(cat))\n",
    "            else:\n",
    "                w_heads = self.qkv_net(cat)\n",
    "            r_head_k = self.r_net(r)\n",
    "\n",
    "            w_head_q, w_head_k, w_head_v = tf.split(w_heads, 3, axis=-1)\n",
    "            w_head_q = w_head_q[-qlen:]\n",
    "        else:\n",
    "            if self.pre_lnorm:\n",
    "                w_heads = self.qkv_net(self.layer_norm(w))\n",
    "            else:\n",
    "                w_heads = self.qkv_net(w)\n",
    "            r_head_k = self.r_net(r)\n",
    "\n",
    "            w_head_q, w_head_k, w_head_v = tf.split(w_heads, 3, axis=-1)\n",
    "\n",
    "        klen = shape_list(w_head_k)[0]\n",
    "\n",
    "        w_head_q = tf.reshape(w_head_q, (qlen, bsz, self.n_head, self.d_head))  # qlen x bsz x n_head x d_head\n",
    "        w_head_k = tf.reshape(w_head_k, (klen, bsz, self.n_head, self.d_head))  # qlen x bsz x n_head x d_head\n",
    "        w_head_v = tf.reshape(w_head_v, (klen, bsz, self.n_head, self.d_head))  # qlen x bsz x n_head x d_head\n",
    "\n",
    "        r_head_k = tf.reshape(r_head_k, (rlen, self.n_head, self.d_head))  # qlen x n_head x d_head\n",
    "\n",
    "        # compute attention score\n",
    "        rw_head_q = w_head_q + self.r_w_bias  # qlen x bsz x n_head x d_head\n",
    "        AC = tf.einsum(\"ibnd,jbnd->ijbn\", rw_head_q, w_head_k)  # qlen x klen x bsz x n_head\n",
    "\n",
    "        rr_head_q = w_head_q + self.r_r_bias\n",
    "        BD = tf.einsum(\"ibnd,jnd->ijbn\", rr_head_q, r_head_k)  # qlen x klen x bsz x n_head\n",
    "        BD = self._rel_shift(BD)\n",
    "\n",
    "        # [qlen x klen x bsz x n_head]\n",
    "        attn_score = AC + BD\n",
    "        attn_score = attn_score * self.scale\n",
    "\n",
    "        # compute attention probability\n",
    "        if attn_mask is not None:\n",
    "            attn_mask_t = attn_mask[:, :, None, None]\n",
    "            attn_score = attn_score * (1 - attn_mask_t) - 1e30 * attn_mask_t\n",
    "\n",
    "        # [qlen x klen x bsz x n_head]\n",
    "        attn_prob = tf.nn.softmax(attn_score, axis=1)\n",
    "        attn_prob = self.dropatt(attn_prob, training=training)\n",
    "\n",
    "        if head_mask is not None:\n",
    "            attn_prob = attn_prob * head_mask\n",
    "\n",
    "        # compute attention vector\n",
    "        attn_vec = tf.einsum(\"ijbn,jbnd->ibnd\", attn_prob, w_head_v)\n",
    "\n",
    "        # [qlen x bsz x n_head x d_head]\n",
    "        attn_vec_sizes = shape_list(attn_vec)\n",
    "        attn_vec = tf.reshape(attn_vec, (attn_vec_sizes[0], attn_vec_sizes[1], self.n_head * self.d_head))\n",
    "\n",
    "        # linear projection\n",
    "        attn_out = self.o_net(attn_vec)\n",
    "        attn_out = self.drop(attn_out, training=training)\n",
    "\n",
    "        if self.pre_lnorm:\n",
    "            # residual connection\n",
    "            outputs = [w + attn_out]\n",
    "        else:\n",
    "            # residual connection + layer normalization\n",
    "            outputs = [self.layer_norm(w + attn_out)]\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs.append(attn_prob)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class TFRelPartialLearnableDecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_head,\n",
    "        d_model,\n",
    "        d_head,\n",
    "        d_inner,\n",
    "        dropout,\n",
    "        dropatt=0.0,\n",
    "        pre_lnorm=False,\n",
    "        r_w_bias=None,\n",
    "        r_r_bias=None,\n",
    "        layer_norm_epsilon=1e-5,\n",
    "        init_std=0.02,\n",
    "        output_attentions=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.dec_attn = TFRelPartialLearnableMultiHeadAttn(\n",
    "            n_head,\n",
    "            d_model,\n",
    "            d_head,\n",
    "            dropout,\n",
    "            dropatt=dropatt,\n",
    "            pre_lnorm=pre_lnorm,\n",
    "            r_w_bias=r_w_bias,\n",
    "            r_r_bias=r_r_bias,\n",
    "            init_std=init_std,\n",
    "            layer_norm_epsilon=layer_norm_epsilon,\n",
    "            output_attentions=output_attentions,\n",
    "            name=\"dec_attn\",\n",
    "        )\n",
    "        self.pos_ff = TFPositionwiseFF(\n",
    "            d_model,\n",
    "            d_inner,\n",
    "            dropout,\n",
    "            pre_lnorm=pre_lnorm,\n",
    "            init_std=init_std,\n",
    "            layer_norm_epsilon=layer_norm_epsilon,\n",
    "            name=\"pos_ff\",\n",
    "        )\n",
    "\n",
    "    def call(self, dec_inp, r, dec_attn_mask, mems, head_mask, output_attentions, training=False):\n",
    "        attn_outputs = self.dec_attn(dec_inp, r, dec_attn_mask, mems, head_mask, output_attentions, training=training)\n",
    "        ff_output = self.pos_ff(attn_outputs[0], training=training)\n",
    "\n",
    "        outputs = [ff_output] + attn_outputs[1:]\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class TFAdaptiveEmbedding(tf.keras.layers.Layer):\n",
    "\"\"\"\n",
    "n개의 token을 한 번에 임베딩하는 것이 아니라 먼저 cutoff를 통해 n개의 tokens을 나누고 mask_i = (inp_flat >= l_idx) & (inp_flat < r_idx) 을 만족하는 토큰들만을 임베딩하는 방식\n",
    "\"\"\"\n",
    "\n",
    "    def __init__(self, n_token, d_embed, d_proj, cutoffs, div_val=1, init_std=0.02, sample_softmax=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.n_token = n_token\n",
    "        self.d_embed = d_embed\n",
    "        self.init_std = init_std\n",
    "\n",
    "        self.cutoffs = cutoffs + [n_token]\n",
    "        self.div_val = div_val\n",
    "        self.d_proj = d_proj\n",
    "\n",
    "        self.emb_scale = d_proj ** 0.5\n",
    "\n",
    "        self.cutoff_ends = [0] + self.cutoffs\n",
    "\n",
    "        self.emb_layers = []\n",
    "        self.emb_projs = []\n",
    "        if div_val == 1:\n",
    "            raise NotImplementedError  # Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint\n",
    "        else:\n",
    "            for i in range(len(self.cutoffs)):\n",
    "                l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]\n",
    "                d_emb_i = d_embed // (div_val ** i)\n",
    "                self.emb_layers.append(\n",
    "                    tf.keras.layers.Embedding(\n",
    "                        r_idx - l_idx,\n",
    "                        d_emb_i,\n",
    "                        embeddings_initializer=get_initializer(init_std),\n",
    "                        name=\"emb_layers_._{}\".format(i),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        for i in range(len(self.cutoffs)):\n",
    "            d_emb_i = self.d_embed // (self.div_val ** i)\n",
    "            self.emb_projs.append(\n",
    "                self.add_weight(\n",
    "                    shape=(d_emb_i, self.d_proj),\n",
    "                    initializer=get_initializer(self.init_std),\n",
    "                    trainable=True,\n",
    "                    name=\"emb_projs_._{}\".format(i),\n",
    "                )\n",
    "            )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inp):\n",
    "        if self.div_val == 1:\n",
    "            raise NotImplementedError  # Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint\n",
    "        else:\n",
    "            inp_flat = tf.reshape(inp, (-1,))\n",
    "            emb_flat = tf.zeros([shape_list(inp_flat)[0], self.d_proj])\n",
    "            for i in range(len(self.cutoffs)):\n",
    "                l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]\n",
    "\n",
    "                mask_i = (inp_flat >= l_idx) & (inp_flat < r_idx)\n",
    "\n",
    "                inp_i = tf.boolean_mask(inp_flat, mask_i) - l_idx\n",
    "                emb_i = self.emb_layers[i](inp_i)\n",
    "                emb_i = tf.einsum(\"id,de->ie\", emb_i, self.emb_projs[i])\n",
    "\n",
    "                mask_idx = tf.cast(tf.where(mask_i), dtype=tf.int64)\n",
    "                emb_flat += tf.scatter_nd(mask_idx, emb_i, tf.cast(shape_list(emb_flat), dtype=tf.int64))\n",
    "\n",
    "            embed_shape = shape_list(inp) + [self.d_proj]\n",
    "            embed = tf.reshape(emb_flat, embed_shape)\n",
    "\n",
    "        embed *= self.emb_scale\n",
    "\n",
    "        return embed\n",
    "\n",
    "\n",
    "@keras_serializable\n",
    "class TFTransfoXLMainLayer(tf.keras.layers.Layer):\n",
    "    config_class = TransfoXLConfig\n",
    "\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.output_hidden_states = config.output_hidden_states\n",
    "        self.output_attentions = config.output_attentions\n",
    "        self.return_dict = config.use_return_dict\n",
    "\n",
    "        self.n_token = config.vocab_size\n",
    "\n",
    "        self.d_embed = config.d_embed\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "        self.untie_r = config.untie_r\n",
    "\n",
    "        self.word_emb = TFAdaptiveEmbedding(\n",
    "            config.vocab_size,\n",
    "            config.d_embed,\n",
    "            config.d_model,\n",
    "            config.cutoffs,\n",
    "            div_val=config.div_val,\n",
    "            init_std=config.init_std,\n",
    "            name=\"word_emb\",\n",
    "        )\n",
    "\n",
    "        self.drop = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "        self.n_layer = config.n_layer\n",
    "        self.mem_len = config.mem_len\n",
    "        self.attn_type = config.attn_type\n",
    "\n",
    "        self.layers = []\n",
    "        if config.attn_type == 0:  # the default attention\n",
    "            for i in range(config.n_layer):\n",
    "                self.layers.append(\n",
    "                    TFRelPartialLearnableDecoderLayer(\n",
    "                        config.n_head,\n",
    "                        config.d_model,\n",
    "                        config.d_head,\n",
    "                        config.d_inner,\n",
    "                        config.dropout,\n",
    "                        dropatt=config.dropatt,\n",
    "                        pre_lnorm=config.pre_lnorm,\n",
    "                        r_w_bias=None if self.untie_r else self.r_w_bias,\n",
    "                        r_r_bias=None if self.untie_r else self.r_r_bias,\n",
    "                        layer_norm_epsilon=config.layer_norm_epsilon,\n",
    "                        init_std=config.init_std,\n",
    "                        output_attentions=self.output_attentions,\n",
    "                        name=\"layers_._{}\".format(i),\n",
    "                    )\n",
    "                )\n",
    "        else:  # learnable embeddings and absolute embeddings\n",
    "            raise NotImplementedError  # Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint\n",
    "\n",
    "        self.same_length = config.same_length\n",
    "        self.clamp_len = config.clamp_len\n",
    "\n",
    "        if self.attn_type == 0:  # default attention\n",
    "            self.pos_emb = TFPositionalEmbedding(self.d_model, name=\"pos_emb\")\n",
    "        else:  # learnable embeddings and absolute embeddings\n",
    "            raise NotImplementedError  # Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if not self.untie_r:\n",
    "            self.r_w_bias = self.add_weight(\n",
    "                shape=(self.n_head, self.d_head), initializer=\"zeros\", trainable=True, name=\"r_w_bias\"\n",
    "            )\n",
    "            self.r_r_bias = self.add_weight(\n",
    "                shape=(self.n_head, self.d_head), initializer=\"zeros\", trainable=True, name=\"r_r_bias\"\n",
    "            )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.word_emb\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _resize_token_embeddings(self, new_num_tokens):\n",
    "        return self.word_emb\n",
    "\n",
    "    def backward_compatible(self):\n",
    "        self.sample_softmax = -1\n",
    "\n",
    "    def reset_memory_length(self, mem_len):\n",
    "        self.mem_len = mem_len\n",
    "\n",
    "    def _prune_heads(self, heads):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def init_mems(self, bsz):\n",
    "        if self.mem_len > 0:\n",
    "            mems = []\n",
    "            for i in range(self.n_layer):\n",
    "                empty = tf.zeros([self.mem_len, bsz, self.d_model])\n",
    "                mems.append(empty)\n",
    "\n",
    "            return mems\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _update_mems(self, hids, mems, mlen, qlen):\n",
    "\t\t\"\"\"\n",
    "\t\t한 칸씩 슬라이딩하며, memory에 새로운 segment를 추가합니다. 이때, tf.stop_gradient를 통해 이전부터 보았던 segment는 gradient가 더 이상 흐르지 않도록 tf.stop_gradient를 사용\n",
    "\t\t\"\"\"\n",
    "        if mems is None:\n",
    "            return None\n",
    "\n",
    "        # mems is not None\n",
    "        assert len(hids) == len(mems), \"len(hids) != len(mems)\"\n",
    "\n",
    "        # `mlen + qlen` steps\n",
    "        new_mems = []\n",
    "        end_idx = mlen + max(0, qlen)\n",
    "        beg_idx = max(0, end_idx - self.mem_len)\n",
    "        for i in range(len(hids)):\n",
    "\n",
    "            cat = tf.concat([mems[i], hids[i]], axis=0)\n",
    "            tf.stop_gradient(cat)\n",
    "            new_mems.append(cat[beg_idx:end_idx])\n",
    "\n",
    "        return new_mems\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        inputs,\n",
    "        mems=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        training=False,\n",
    "    ):\n",
    "        if isinstance(inputs, (tuple, list)):\n",
    "            input_ids = inputs[0]\n",
    "            mems = inputs[1] if len(inputs) > 1 else mems\n",
    "            head_mask = inputs[2] if len(inputs) > 2 else head_mask\n",
    "            inputs_embeds = inputs[3] if len(inputs) > 3 else inputs_embeds\n",
    "            output_attentions = inputs[4] if len(inputs) > 4 else output_attentions\n",
    "            output_hidden_states = inputs[5] if len(inputs) > 5 else output_hidden_states\n",
    "            return_dict = inputs[6] if len(inputs) > 6 else return_dict\n",
    "            assert len(inputs) <= 7, \"Too many inputs.\"\n",
    "        elif isinstance(inputs, (dict, BatchEncoding)):\n",
    "            input_ids = inputs.get(\"input_ids\")\n",
    "            mems = inputs.get(\"mems\", mems)\n",
    "            head_mask = inputs.get(\"head_mask\", head_mask)\n",
    "            inputs_embeds = inputs.get(\"inputs_embeds\", inputs_embeds)\n",
    "            output_attentions = inputs.get(\"output_attentions\", output_attentions)\n",
    "            output_hidden_states = inputs.get(\"output_hidden_states\", output_hidden_states)\n",
    "            return_dict = inputs.get(\"return_dict\", return_dict)\n",
    "            assert len(inputs) <= 7, \"Too many inputs.\"\n",
    "        else:\n",
    "            input_ids = inputs\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.output_attentions\n",
    "        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.output_hidden_states\n",
    "        return_dict = return_dict if return_dict is not None else self.return_dict\n",
    "\n",
    "        # the original code for Transformer-XL used shapes [len, bsz] but we want a unified interface in the library\n",
    "        # so we transpose here from shape [bsz, len] to shape [len, bsz]\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_ids = tf.transpose(input_ids, perm=(1, 0))\n",
    "            qlen, bsz = shape_list(input_ids)\n",
    "        elif inputs_embeds is not None:\n",
    "            inputs_embeds = tf.transpose(inputs_embeds, perm=(1, 0, 2))\n",
    "            qlen, bsz = shape_list(inputs_embeds)[:2]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        if mems is None:\n",
    "            mems = self.init_mems(bsz)\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads] (a head_mask for each layer)\n",
    "        # and head_mask is converted to shape [num_hidden_layers x qlen x klen x bsz x n_head]\n",
    "        if head_mask is not None:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            head_mask = [None] * self.n_layer\n",
    "\n",
    "        if inputs_embeds is not None:\n",
    "            word_emb = inputs_embeds\n",
    "        else:\n",
    "            word_emb = self.word_emb(input_ids)\n",
    "\n",
    "        mlen = shape_list(mems[0])[0] if mems is not None else 0\n",
    "        klen = mlen + qlen\n",
    "\n",
    "        attn_mask = tf.ones([qlen, qlen])\n",
    "        mask_u = tf.linalg.band_part(attn_mask, 0, -1)\n",
    "        mask_dia = tf.linalg.band_part(attn_mask, 0, 0)\n",
    "        attn_mask_pad = tf.zeros([qlen, mlen])\n",
    "        dec_attn_mask = tf.concat([attn_mask_pad, mask_u - mask_dia], 1)\n",
    "        if self.same_length:\n",
    "            mask_l = tf.linalg.band_part(attn_mask, -1, 0)\n",
    "            dec_attn_mask = tf.concat([dec_attn_mask[:, :qlen] + mask_l - mask_dia, dec_attn_mask[:, qlen:]], 1)\n",
    "        # ::: PyTorch masking code for reference :::\n",
    "        # if self.same_length:\n",
    "        #     all_ones = word_emb.new_ones((qlen, klen), dtype=torch.uint8)\n",
    "        #     mask_len = klen - self.mem_len\n",
    "        #     if mask_len > 0:\n",
    "        #         mask_shift_len = qlen - mask_len\n",
    "        #     else:\n",
    "        #         mask_shift_len = qlen\n",
    "        #     dec_attn_mask = (torch.triu(all_ones, 1+mlen)\n",
    "        #             + torch.tril(all_ones, -mask_shift_len))[:, :, None] # -1\n",
    "        # else:\n",
    "        #     dec_attn_mask = torch.triu(\n",
    "        #         word_emb.new_ones((qlen, klen), dtype=torch.uint8), diagonal=1+mlen)[:,:,None]\n",
    "\n",
    "        hids = []\n",
    "        attentions = [] if output_attentions else None\n",
    "        if self.attn_type == 0:  # default\n",
    "            pos_seq = tf.range(klen - 1, -1, -1.0)\n",
    "            if self.clamp_len > 0:\n",
    "                pos_seq = tf.minimum(pos_seq, self.clamp_len)\n",
    "            pos_emb = self.pos_emb(pos_seq)\n",
    "\n",
    "            core_out = self.drop(word_emb, training=training)\n",
    "            pos_emb = self.drop(pos_emb, training=training)\n",
    "\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                hids.append(core_out)\n",
    "                mems_i = None if mems is None else mems[i]\n",
    "                layer_outputs = layer(\n",
    "                    core_out,\n",
    "                    pos_emb,\n",
    "                    dec_attn_mask,\n",
    "                    mems_i,\n",
    "                    head_mask[i],\n",
    "                    output_attentions,\n",
    "                    training=training,\n",
    "                )\n",
    "                core_out = layer_outputs[0]\n",
    "                if output_attentions:\n",
    "                    attentions.append(layer_outputs[1])\n",
    "        else:  # learnable embeddings and absolute embeddings\n",
    "            raise NotImplementedError\n",
    "\n",
    "        core_out = self.drop(core_out, training=training)\n",
    "\n",
    "        new_mems = self._update_mems(hids, mems, mlen, qlen)\n",
    "\n",
    "        # [bsz, len, hidden_dim]\n",
    "        core_out = tf.transpose(core_out, perm=(1, 0, 2))\n",
    "\n",
    "        if output_hidden_states:\n",
    "            # last layer를 추가하고 다시 library standard shape [bsz, len, hidden_dim]으로 transpose\n",
    "            hids.append(core_out)\n",
    "            hids = tuple(tf.transpose(t, perm=(1, 0, 2)) for t in hids)\n",
    "        else:\n",
    "            hids = None\n",
    "        if output_attentions:\n",
    "            # Transpose to library standard shape [bsz, n_heads, query_seq_len, key_seq_len]\n",
    "            attentions = tuple(tf.transpose(t, perm=(2, 3, 0, 1)) for t in attentions)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [core_out, new_mems, hids, attentions] if v is not None)\n",
    "\n",
    "        return TFTransfoXLModelOutput(\n",
    "            last_hidden_state=core_out,\n",
    "            mems=new_mems,\n",
    "            hidden_states=hids,\n",
    "            attentions=attentions,\n",
    "        )\n",
    "\n",
    "class TFAdaptiveSoftmaxMask(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_embed, d_proj, cutoffs, div_val=1, keep_order=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_embed = d_embed\n",
    "        self.d_proj = d_proj\n",
    "\n",
    "        self.cutoffs = cutoffs + [vocab_size]\n",
    "        self.cutoff_ends = [0] + self.cutoffs\n",
    "        self.div_val = div_val\n",
    "\n",
    "        self.shortlist_size = self.cutoffs[0]\n",
    "        self.n_clusters = len(self.cutoffs) - 1\n",
    "        self.head_size = self.shortlist_size + self.n_clusters\n",
    "        self.keep_order = keep_order\n",
    "\n",
    "        self.out_layers = []\n",
    "        self.out_projs = []\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.n_clusters > 0:\n",
    "            self.cluster_weight = self.add_weight(\n",
    "                shape=(self.n_clusters, self.d_embed), initializer=\"zeros\", trainable=True, name=\"cluster_weight\"\n",
    "            )\n",
    "            self.cluster_bias = self.add_weight(\n",
    "                shape=(self.n_clusters,), initializer=\"zeros\", trainable=True, name=\"cluster_bias\"\n",
    "            )\n",
    "\n",
    "        if self.div_val == 1:\n",
    "            for i in range(len(self.cutoffs)):\n",
    "                if self.d_proj != self.d_embed:\n",
    "                    weight = self.add_weight(\n",
    "                        shape=(self.d_embed, self.d_proj),\n",
    "                        initializer=\"zeros\",\n",
    "                        trainable=True,\n",
    "                        name=\"out_projs_._{}\".format(i),\n",
    "                    )\n",
    "                    self.out_projs.append(weight)\n",
    "                else:\n",
    "                    self.out_projs.append(None)\n",
    "                weight = self.add_weight(\n",
    "                    shape=(\n",
    "                        self.vocab_size,\n",
    "                        self.d_embed,\n",
    "                    ),\n",
    "                    initializer=\"zeros\",\n",
    "                    trainable=True,\n",
    "                    name=\"out_layers_._{}_._weight\".format(i),\n",
    "                )\n",
    "                bias = self.add_weight(\n",
    "                    shape=(self.vocab_size,),\n",
    "                    initializer=\"zeros\",\n",
    "                    trainable=True,\n",
    "                    name=\"out_layers_._{}_._bias\".format(i),\n",
    "                )\n",
    "                self.out_layers.append((weight, bias))\n",
    "        else:\n",
    "            for i in range(len(self.cutoffs)):\n",
    "                l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]\n",
    "                d_emb_i = self.d_embed // (self.div_val ** i)\n",
    "\n",
    "                weight = self.add_weight(\n",
    "                    shape=(d_emb_i, self.d_proj), initializer=\"zeros\", trainable=True, name=\"out_projs_._{}\".format(i)\n",
    "                )\n",
    "                self.out_projs.append(weight)\n",
    "                weight = self.add_weight(\n",
    "                    shape=(\n",
    "                        r_idx - l_idx,\n",
    "                        d_emb_i,\n",
    "                    ),\n",
    "                    initializer=\"zeros\",\n",
    "                    trainable=True,\n",
    "                    name=\"out_layers_._{}_._weight\".format(i),\n",
    "                )\n",
    "                bias = self.add_weight(\n",
    "                    shape=(r_idx - l_idx,),\n",
    "                    initializer=\"zeros\",\n",
    "                    trainable=True,\n",
    "                    name=\"out_layers_._{}_._bias\".format(i),\n",
    "                )\n",
    "                self.out_layers.append((weight, bias))\n",
    "        super().build(input_shape)\n",
    "\n",
    "    @staticmethod\n",
    "    def _logit(x, W, b, proj=None):\n",
    "        y = x\n",
    "        if proj is not None:\n",
    "            y = tf.einsum(\"ibd,ed->ibe\", y, proj)\n",
    "        return tf.einsum(\"ibd,nd->ibn\", y, W) + b\n",
    "\n",
    "    @staticmethod\n",
    "    def _gather_logprob(logprob, target):\n",
    "        lp_size = shape_list(logprob)\n",
    "        r = tf.range(lp_size[0])\n",
    "        idx = tf.stack([r, target], 1)\n",
    "        return tf.gather_nd(logprob, idx)\n",
    "\n",
    "    def call(self, hidden, target, return_mean=True, training=False):\n",
    "        head_logprob = 0\n",
    "        if self.n_clusters == 0:\n",
    "            output = self._logit(hidden, self.out_layers[0][0], self.out_layers[0][1], self.out_projs[0])\n",
    "            if target is not None:\n",
    "                loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=target, logits=output)\n",
    "            out = tf.nn.log_softmax(output, axis=-1)\n",
    "        else:\n",
    "            hidden_sizes = shape_list(hidden)\n",
    "            out = []\n",
    "            loss = tf.zeros(hidden_sizes[:2], dtype=tf.float32)\n",
    "            for i in range(len(self.cutoffs)):\n",
    "                l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]\n",
    "                if target is not None:\n",
    "                    mask = (target >= l_idx) & (target < r_idx)\n",
    "                    mask_idx = tf.where(mask)\n",
    "                    cur_target = tf.boolean_mask(target, mask) - l_idx\n",
    "\n",
    "                if self.div_val == 1:\n",
    "                    cur_W = self.out_layers[0][0][l_idx:r_idx]\n",
    "                    cur_b = self.out_layers[0][1][l_idx:r_idx]\n",
    "                else:\n",
    "                    cur_W = self.out_layers[i][0]\n",
    "                    cur_b = self.out_layers[i][1]\n",
    "\n",
    "                if i == 0:\n",
    "                    cur_W = tf.concat([cur_W, self.cluster_weight], 0)\n",
    "                    cur_b = tf.concat([cur_b, self.cluster_bias], 0)\n",
    "\n",
    "                    head_logit = self._logit(hidden, cur_W, cur_b, self.out_projs[0])\n",
    "                    head_logprob = tf.nn.log_softmax(head_logit)\n",
    "                    out.append(head_logprob[..., : self.cutoffs[0]])\n",
    "                    if target is not None:\n",
    "                        cur_head_logprob = tf.boolean_mask(head_logprob, mask)\n",
    "                        cur_logprob = self._gather_logprob(cur_head_logprob, cur_target)\n",
    "                else:\n",
    "                    tail_logit = self._logit(hidden, cur_W, cur_b, self.out_projs[i])\n",
    "                    tail_logprob = tf.nn.log_softmax(tail_logit)\n",
    "                    cluster_prob_idx = self.cutoffs[0] + i - 1  # No probability for the head cluster\n",
    "                    logprob_i = head_logprob[..., cluster_prob_idx, None] + tail_logprob\n",
    "                    out.append(logprob_i)\n",
    "                    if target is not None:\n",
    "                        cur_head_logprob = tf.boolean_mask(head_logprob, mask)\n",
    "                        cur_tail_logprob = tf.boolean_mask(tail_logprob, mask)\n",
    "                        cur_logprob = self._gather_logprob(cur_tail_logprob, cur_target)\n",
    "                        cur_logprob += cur_head_logprob[:, self.cutoff_ends[1] + i - 1]\n",
    "                if target is not None:\n",
    "                    loss += tf.scatter_nd(mask_idx, -cur_logprob, tf.cast(shape_list(loss), dtype=tf.int64))\n",
    "            out = tf.concat(out, axis=-1)\n",
    "\n",
    "        if target is not None:\n",
    "            if return_mean:\n",
    "                loss = tf.reduce_mean(loss)\n",
    "            # `self.add_loss()`를 통해 training시의 loss 추가\n",
    "            self.add_loss(loss)\n",
    "\n",
    "            # Log the loss as a metric\n",
    "            self.add_metric(loss, name=self.name, aggregation=\"mean\" if return_mean else \"\")\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerous-tribute",
   "metadata": {},
   "source": [
    "- TFTransfoXLMainLayer이라는 메인 클래스 안에서 Transformer-XL의 주요 특징인 State Reuse를 위한 메모리 관리가 구현된 _update_mems() 메소드, 그리고 TFRelPartialLearnableMultiHeadAttn 레이어에 구현된 Relative attention이 TFRelPartialLearnableDecoderLayer 안에서 어떻게 recurrent하게 사용되는지를 눈여겨 봐주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-neighbor",
   "metadata": {},
   "source": [
    "## 13-7 XLNET \n",
    "- transformer-XL을 이용한 아키텍처\n",
    "-  GPT의 AR(AutoRegressive) 언어 모델과 BERT의 AE(AutoEncoding) 언어 모델과는 다른 퍼뮤테이션(Permutation) 언어 모델을 통해 더욱 정교한 언어 모델 성능을 선보이고 있습니다.\n",
    "- <a href=\"https://ratsgo.github.io/natural%20language%20processing/2019/09/11/xlnet/\" target=\"_blank\"> XLNET 블로그 읽어보기</a>\n",
    "- <a href=\"https://arxiv.org/abs/1906.08237\" target=\"_blank\"> XLNET 논문 읽어보기</a>\n",
    "\n",
    "[논문의 LM의 각각의 흐름과 단점을 정리]\n",
    "- 두 가지 흐름은 AR(auto-regressive)과 AE(auto-encoder)입니다.\n",
    "\n",
    "- AR의 경우 GPT와 같은 일반적인 LM을 의미하며, 이는 BERT에서도 지적했듯이 한 방향으로 학습한다는 것입니다. 양방향으로 텍스트를 볼 수 없다는 것이 context를 습득하는 데에 있어서 단점으로 작용합니다.\n",
    "\n",
    "- 또 다른 흐름인 AE는 BERT로 대표될 수 있습니다. AE는 모델에 노이즈를 주고 이를 복원하는 방식으로 학습합니다. \n",
    "- BERT는 MASK 토큰을 통해 모델의 노이즈를 주었으며, 이를 맞추는 과정에서 학습을 하게 됩니다. \n",
    "- AR과 달리 양방향으로 context를 볼 수 있다는 장점이 있지만, pretrain시에 존재하던 MASK토큰이 finetuning과 evaluation 시에 보이지 않아 불일치가 발생한다는 것입니다. \n",
    "- AE의 더 큰 문제는, MASK토큰들 간의 dependency를 확인할 수 없다는 문제가 있었습니다.\n",
    "\n",
    "[permutation LM 효과]\n",
    "- permutation LM을 사용함으로써 XLNET은 AR과 AE의 장점을 동시에 가질 수 있습니다. 즉, AE처럼 양방향 context를 모두 볼 수 있는 동시에, AR처럼 예측해야 할 토큰들 간의 dependency를 놓치지 않고 학습할 수 있게 됩니다.\n",
    "\n",
    "[두 가지 흐름의 attention 메커니즘이 적용]\n",
    "\n",
    "- XLNET에는 content stream attention과 query stream attention이 사용됩니다. \n",
    "- content stream attention은 기존의 트랜스포머의 attention과 동일한 attention입니다. \n",
    "- query stream attention은 현재 토큰에서 position 정보만을 이용하는 attention입니다. \n",
    "- 현재 토큰의 content를 제외시킴으로써 동일한 input으로 다른 ouput을 내야 한다는 permutation LM이 가지는 문제점을 해결해줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-customs",
   "metadata": {},
   "source": [
    "## ALBERT(A Lite BERT for Self-supervised Learning of Language Representations)\n",
    "- 목표 : 성능은 유지하면서 메모리는 적게 쓰는 좀 더 가벼운 BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-spelling",
   "metadata": {},
   "source": [
    "### ALBERT의 구조\n",
    "- - <a href=\"https://arxiv.org/abs/1909.11942\" target=\"_blank\"> ALBERT 논문 읽어보기</a>\n",
    "- 트랜스포머의 encoder를 기반으로 하며, activation function(활성화 함수)으로는 GELU를 사용\n",
    "\n",
    "- 1) : 1. Factorized embedding parameterization\n",
    "     - BERT는 input의 token embedding(E)과 hidden layer(H)의 size가 같다.(E≡H) ALBERT는 E를 H보다 작게 만들어서 parameter의 수를 줄입니다. *hidden layer(H) 사이즈가 클수록 성능이 높아질 가능성이 있고, E의 사이즈를 줄여도 성능과는 상관이 없어 보인다고 함\n",
    "     - matrix factorization(행렬 분해)를 통해  parameter의 수도 줄일 수 있다.\n",
    "- 2) : 2. Cross-layer parameter sharing\n",
    "    - layer간의 모든 parameter들을 공유하여  parameter의 수를 줄인다. ALBERT는 transformer block 1개를 이용하여 재사용\n",
    "- 3) : 3. Inter-sentence coherence loss\n",
    "    -  BERT의 Next Sentence Prediction(NSP) 를 <B> Sentence Order Prediction(SOP)로 대체</B> \n",
    "         - 실제 두 문장의 순서를 바꾸어 학습 데이터를 두 문장의 순서가 원래의 데이터의 순서와 일치한다면 positive, 순서가 원래 데이터와 반대로 되어있다면 negative이 되는 것이죠."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-starter",
   "metadata": {},
   "source": [
    "####  ALBERT의 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-damages",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFAlbertEmbeddings(tf.keras.layers.Layer):\n",
    "    \"\"\"word, position and token_type embeddings\"\"\"\n",
    "\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.embedding_size = config.embedding_size \n",
    "        self.initializer_range = config.initializer_range\n",
    "        self.max_position_embeddings = config.max_position_embeddings\n",
    "        self.type_vocab_size = config.type_vocab_size\n",
    "        self.layer_norm_eps = config.layer_norm_eps\n",
    "        self.hidden_dropout_prob = config.hidden_dropout_prob\n",
    "\n",
    "        ## Albert에서는 hidden_size가 아닌 embedding_size로 embedding을 함\n",
    "        self.position_embeddings = tf.keras.layers.Embedding(\n",
    "            self.max_position_embeddings,\n",
    "            self.embedding_size,\n",
    "            embeddings_initializer=get_initializer(self.initializer_range),\n",
    "            name=\"position_embeddings\",\n",
    "        )\n",
    "        self.token_type_embeddings = tf.keras.layers.Embedding(\n",
    "            self.type_vocab_size,\n",
    "            self.embedding_size,\n",
    "            embeddings_initializer=get_initializer(self.initializer_range),\n",
    "            name=\"token_type_embeddings\",\n",
    "        )\n",
    "\n",
    "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=self.layer_norm_eps, name=\"LayerNorm\")\n",
    "        self.dropout = tf.keras.layers.Dropout(self.hidden_dropout_prob)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"shared word embedding layer \"\"\"\n",
    "        with tf.name_scope(\"word_embeddings\"):\n",
    "            self.word_embeddings = self.add_weight(\n",
    "                \"weight\",\n",
    "                shape=[self.vocab_size, self.embedding_size],\n",
    "                initializer=get_initializer(self.initializer_range),\n",
    "            )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        position_ids=None,\n",
    "        token_type_ids=None,\n",
    "        inputs_embeds=None,\n",
    "        mode=\"embedding\",\n",
    "        training=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        input의 token embeddings\n",
    "        Args:\n",
    "            inputs: int64 tensors (shape [batch_size, length]) 3개를 담은 리스트: (input_ids, position_ids, token_type_ids)\n",
    "            mode: \"embedding\" | \"linear\"\n",
    "        Returns:\n",
    "            outputs: mode == \"embedding\"; output embedding tensor(float32, shape [batch_size, length, embedding_size])\n",
    "\t\t\t\t\t\t\t\t\t\t mode == \"linear\", output linear tensor(float32, shape [batch_size, length, vocab_size])\n",
    "        Raises:\n",
    "            ValueError: if mode is not valid.\n",
    "\t\t\t\t\"\"\"\n",
    "\n",
    "        if mode == \"embedding\":\n",
    "            return self._embedding(input_ids, position_ids, token_type_ids, inputs_embeds, training=training)\n",
    "        elif mode == \"linear\":\n",
    "            return self._linear(input_ids)\n",
    "        else:\n",
    "            raise ValueError(\"mode {} is not valid.\".format(mode))\n",
    "\n",
    "    def _embedding(self, input_ids, position_ids, token_type_ids, inputs_embeds, training=False):\n",
    "        \"\"\"input tensor에 기반하여 임베딩 적용\"\"\"\n",
    "        assert not (input_ids is None and inputs_embeds is None)\n",
    "\n",
    "        if input_ids is not None:\n",
    "            input_shape = shape_list(input_ids)\n",
    "        else:\n",
    "            input_shape = shape_list(inputs_embeds)[:-1]\n",
    "\n",
    "        seq_length = input_shape[1]\n",
    "        if position_ids is None:\n",
    "            position_ids = tf.range(seq_length, dtype=tf.int32)[tf.newaxis, :]\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = tf.fill(input_shape, 0)\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = tf.gather(self.word_embeddings, input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = inputs_embeds + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings, training=training)\n",
    "        return embeddings\n",
    "\n",
    "    def _linear(self, inputs):\n",
    "        \"\"\"\n",
    " \t\t\t  linear layer를 통해서 input의 logit을 계산\n",
    "        Args:\n",
    "            inputs: float32 tensor (shape [batch_size, length, embedding_size])\n",
    "        Returns:\n",
    "            float32 tensor (shape [batch_size, length, vocab_size])\n",
    "        \"\"\"\n",
    "        batch_size = shape_list(inputs)[0]\n",
    "        length = shape_list(inputs)[1]\n",
    "        x = tf.reshape(inputs, [-1, self.embedding_size])\n",
    "        logits = tf.matmul(x, self.word_embeddings, transpose_b=True)\n",
    "        return tf.reshape(logits, [batch_size, length, self.vocab_size])\n",
    "\n",
    "\n",
    "class TFAlbertSelfOutput(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n",
    "        )\n",
    "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n",
    "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def call(self, hidden_states, input_tensor, training=False):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states, training=training)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class TFAlbertAttention(tf.keras.layers.Layer):\n",
    "    \"\"\" dropouts and layer norm을 포함한 attention layer \"\"\"\n",
    "\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.output_attentions = config.output_attentions\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        assert config.hidden_size % config.num_attention_heads == 0\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        self.query = tf.keras.layers.Dense(\n",
    "            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"query\"\n",
    "        )\n",
    "        self.key = tf.keras.layers.Dense(\n",
    "            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"key\"\n",
    "        )\n",
    "        self.value = tf.keras.layers.Dense(\n",
    "            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"value\"\n",
    "        )\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n",
    "        )\n",
    "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n",
    "        self.pruned_heads = set()\n",
    "        # Two different dropout probabilities; see https://github.com/google-research/albert/blob/master/modeling.py#L971-L993\n",
    "        self.attention_dropout = tf.keras.layers.Dropout(config.attention_probs_dropout_prob)\n",
    "        self.output_dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_attention_heads, self.attention_head_size))\n",
    "\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def prune_heads(self, heads):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def call(self, input_tensor, attention_mask, head_mask, output_attentions, training=False):\n",
    "        batch_size = shape_list(input_tensor)[0]\n",
    "        mixed_query_layer = self.query(input_tensor)\n",
    "        mixed_key_layer = self.key(input_tensor)\n",
    "        mixed_value_layer = self.value(input_tensor)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer, batch_size)\n",
    "\n",
    "        # \"query\"와 \"key\"의 dot product : raw attention scores\n",
    "        # (batch size, num_heads, seq_len_q, seq_len_k)\n",
    "        attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n",
    "        # scale attention_scores\n",
    "        dk = tf.cast(shape_list(key_layer)[-1], tf.float32)\n",
    "        attention_scores = attention_scores / tf.math.sqrt(dk)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = tf.nn.softmax(attention_scores, axis=-1)\n",
    "\n",
    "        attention_probs = self.attention_dropout(attention_probs, training=training)\n",
    "\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        context_layer = tf.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = tf.transpose(context_layer, perm=[0, 2, 1, 3])\n",
    "        context_layer = tf.reshape(\n",
    "            context_layer, (batch_size, -1, self.all_head_size)\n",
    "        )  # (batch_size, seq_len_q, all_head_size)\n",
    "\n",
    "        self_outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "\n",
    "        hidden_states = self_outputs[0]\n",
    "\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.output_dropout(hidden_states, training=training)\n",
    "        attention_output = self.LayerNorm(hidden_states + input_tensor)\n",
    "\n",
    "        outputs = (attention_output,) + self_outputs[1:]\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class TFAlbertLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.attention = TFAlbertAttention(config, name=\"attention\")\n",
    "\n",
    "        self.ffn = tf.keras.layers.Dense(\n",
    "            config.intermediate_size, kernel_initializer=get_initializer(config.initializer_range), name=\"ffn\"\n",
    "        )\n",
    "\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.activation = get_tf_activation(config.hidden_act)\n",
    "        else:\n",
    "            self.activation = config.hidden_act\n",
    "\n",
    "        self.ffn_output = tf.keras.layers.Dense(\n",
    "            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"ffn_output\"\n",
    "        )\n",
    "        self.full_layer_layer_norm = tf.keras.layers.LayerNormalization(\n",
    "            epsilon=config.layer_norm_eps, name=\"full_layer_layer_norm\"\n",
    "        )\n",
    "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def call(self, hidden_states, attention_mask, head_mask, output_attentions, training=False):\n",
    "        attention_outputs = self.attention(\n",
    "            hidden_states, attention_mask, head_mask, output_attentions, training=training\n",
    "        )\n",
    "        ffn_output = self.ffn(attention_outputs[0])\n",
    "        ffn_output = self.activation(ffn_output)\n",
    "        ffn_output = self.ffn_output(ffn_output)\n",
    "        ffn_output = self.dropout(ffn_output, training=training)\n",
    "\n",
    "        hidden_states = self.full_layer_layer_norm(ffn_output + attention_outputs[0])\n",
    "\n",
    "        outputs = (hidden_states,) + attention_outputs[1:]\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class TFAlbertLayerGroup(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.output_attentions = config.output_attentions\n",
    "        self.output_hidden_states = config.output_hidden_states\n",
    "        self.albert_layers = [\n",
    "            TFAlbertLayer(config, name=\"albert_layers_._{}\".format(i)) for i in range(config.inner_group_num)\n",
    "        ]\n",
    "\n",
    "    def call(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, training=False):\n",
    "        layer_hidden_states = ()\n",
    "        layer_attentions = ()\n",
    "\n",
    "        for layer_index, albert_layer in enumerate(self.albert_layers):\n",
    "            layer_output = albert_layer(\n",
    "                hidden_states, attention_mask, head_mask[layer_index], output_attentions, training=training\n",
    "            )\n",
    "            hidden_states = layer_output[0]\n",
    "\n",
    "            if output_attentions:\n",
    "                layer_attentions = layer_attentions + (layer_output[1],)\n",
    "\n",
    "            if output_hidden_states:\n",
    "                layer_hidden_states = layer_hidden_states + (hidden_states,)\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "        if output_hidden_states:\n",
    "            outputs = outputs + (layer_hidden_states,)\n",
    "        if output_attentions:\n",
    "            outputs = outputs + (layer_attentions,)\n",
    "        # last-layer hidden state, (layer hidden states), (layer attentions)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class TFAlbertTransformer(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.num_hidden_layers = config.num_hidden_layers\n",
    "        self.num_hidden_groups = config.num_hidden_groups\n",
    "        self.embedding_hidden_mapping_in = tf.keras.layers.Dense(\n",
    "            config.hidden_size,\n",
    "            kernel_initializer=get_initializer(config.initializer_range),\n",
    "            name=\"embedding_hidden_mapping_in\",\n",
    "        )\n",
    "        self.albert_layer_groups = [\n",
    "            TFAlbertLayerGroup(config, name=\"albert_layer_groups_._{}\".format(i))\n",
    "            for i in range(config.num_hidden_groups)\n",
    "        ]\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask,\n",
    "        head_mask,\n",
    "        output_attentions,\n",
    "        output_hidden_states,\n",
    "        return_dict,\n",
    "        training=False,\n",
    "    ):\n",
    "        hidden_states = self.embedding_hidden_mapping_in(hidden_states)\n",
    "        all_attentions = () if output_attentions else None\n",
    "        all_hidden_states = (hidden_states,) if output_hidden_states else None\n",
    "\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            # Number of layers in a hidden group\n",
    "            layers_per_group = int(self.num_hidden_layers / self.num_hidden_groups)\n",
    "\n",
    "            # Index of the hidden group\n",
    "            group_idx = int(i / (self.num_hidden_layers / self.num_hidden_groups))\n",
    "\n",
    "            layer_group_output = self.albert_layer_groups[group_idx](\n",
    "                hidden_states,\n",
    "                attention_mask,\n",
    "                head_mask[group_idx * layers_per_group : (group_idx + 1) * layers_per_group],\n",
    "                output_attentions,\n",
    "                output_hidden_states,\n",
    "                training=training,\n",
    "            )\n",
    "            hidden_states = layer_group_output[0]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_attentions = all_attentions + layer_group_output[-1]\n",
    "\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, all_hidden_states, all_attentions] if v is not None)\n",
    "        return TFBaseModelOutput(\n",
    "            last_hidden_state=hidden_states, hidden_states=all_hidden_states, attentions=all_attentions\n",
    "        )\n",
    "\n",
    "@keras_serializable\n",
    "class TFAlbertMainLayer(tf.keras.layers.Layer):\n",
    "\"\"\"\n",
    "모델의 전체구조\n",
    "\"\"\"\n",
    "    config_class = AlbertConfig\n",
    "\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_hidden_layers = config.num_hidden_layers\n",
    "        self.output_attentions = config.output_attentions\n",
    "        self.output_hidden_states = config.output_hidden_states\n",
    "        self.return_dict = config.use_return_dict\n",
    "\n",
    "        self.embeddings = TFAlbertEmbeddings(config, name=\"embeddings\")\n",
    "        self.encoder = TFAlbertTransformer(config, name=\"encoder\")\n",
    "        self.pooler = tf.keras.layers.Dense(\n",
    "            config.hidden_size,\n",
    "            kernel_initializer=get_initializer(config.initializer_range),\n",
    "            activation=\"tanh\",\n",
    "            name=\"pooler\",\n",
    "        )\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embeddings\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embeddings.word_embeddings = value\n",
    "        self.embeddings.vocab_size = value.shape[0]\n",
    "\n",
    "    def _resize_token_embeddings(self, new_num_tokens):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        inputs,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        training=False,\n",
    "    ):\n",
    "        if isinstance(inputs, (tuple, list)):\n",
    "            input_ids = inputs[0]\n",
    "            attention_mask = inputs[1] if len(inputs) > 1 else attention_mask\n",
    "            token_type_ids = inputs[2] if len(inputs) > 2 else token_type_ids\n",
    "            position_ids = inputs[3] if len(inputs) > 3 else position_ids\n",
    "            head_mask = inputs[4] if len(inputs) > 4 else head_mask\n",
    "            inputs_embeds = inputs[5] if len(inputs) > 5 else inputs_embeds\n",
    "            output_attentions = inputs[6] if len(inputs) > 6 else output_attentions\n",
    "            output_hidden_states = inputs[7] if len(inputs) > 7 else output_hidden_states\n",
    "            return_dict = inputs[8] if len(inputs) > 8 else return_dict\n",
    "            assert len(inputs) <= 9, \"Too many inputs.\"\n",
    "        elif isinstance(inputs, (dict, BatchEncoding)):\n",
    "            input_ids = inputs.get(\"input_ids\")\n",
    "            attention_mask = inputs.get(\"attention_mask\", attention_mask)\n",
    "            token_type_ids = inputs.get(\"token_type_ids\", token_type_ids)\n",
    "            position_ids = inputs.get(\"position_ids\", position_ids)\n",
    "            head_mask = inputs.get(\"head_mask\", head_mask)\n",
    "            inputs_embeds = inputs.get(\"inputs_embeds\", inputs_embeds)\n",
    "            output_attentions = inputs.get(\"output_attentions\", output_attentions)\n",
    "            output_hidden_states = inputs.get(\"output_hidden_states\", output_hidden_states)\n",
    "            return_dict = inputs.get(\"return_dict\", return_dict)\n",
    "            assert len(inputs) <= 9, \"Too many inputs.\"\n",
    "        else:\n",
    "            input_ids = inputs\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.output_attentions\n",
    "        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.output_hidden_states\n",
    "        return_dict = return_dict if return_dict is not None else self.return_dict\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = shape_list(input_ids)\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = shape_list(inputs_embeds)[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = tf.fill(input_shape, 1)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = tf.fill(input_shape, 0)\n",
    "\n",
    "        # 3D attention mask 만들기\n",
    "        # Sizes : [batch_size, 1, 1, to_seq_length]\n",
    "        # 3D attention mask를 [batch_size, num_heads, from_seq_length, to_seq_length]로 브로드캐스팅\n",
    "\n",
    "        extended_attention_mask = attention_mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "\t\t\t\t# attention_mask가 1.0이면 포함, 0.0이면 미포함 하여 attention 계산\n",
    "        extended_attention_mask = tf.cast(extended_attention_mask, tf.float32)\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        # head_mask가 1.0이면, head를 유지\n",
    "        # attention_probs : shape bsz x n_heads x N x N\n",
    "        # input head_mask : shape [num_heads] 또는 [num_hidden_layers x num_heads]\n",
    "        # head_mask : shape [num_hidden_layers x batch x num_heads x seq_length x seq_length\n",
    "        if head_mask is not None:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            head_mask = [None] * self.num_hidden_layers\n",
    "            # head_mask = tf.constant([0] * self.num_hidden_layers)\n",
    "\n",
    "        embedding_output = self.embeddings(input_ids, position_ids, token_type_ids, inputs_embeds, training=training)\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            extended_attention_mask,\n",
    "            head_mask,\n",
    "            output_attentions,\n",
    "            output_hidden_states,\n",
    "            return_dict,\n",
    "            training=training,\n",
    "        )\n",
    "\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output[:, 0])\n",
    "\n",
    "        if not return_dict:\n",
    "            return (\n",
    "                sequence_output,\n",
    "                pooled_output,\n",
    "            ) + encoder_outputs[1:]\n",
    "\n",
    "        return TFBaseModelOutputWithPooling(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooled_output,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-version",
   "metadata": {},
   "source": [
    "- 얼핏 BERT와 유사한 구조의 코드로 보이지만, TFAlbertEmbeddings의 임베딩 방식과 TFAlbertTransformer 안에서 group 단위로 파라미터가 재사용되는 부분을 유심히 살펴봐 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-basis",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
