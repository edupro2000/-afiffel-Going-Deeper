{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "protected-parker",
   "metadata": {},
   "source": [
    "### 디렉토리 생성 및 나눔 글꼴 설치\n",
    "\n",
    "-  mkdir -p ~/aiffel/transformer\n",
    "-  sudo apt -qq -y install fonts-nanum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "minimal-rwanda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 한글 실행시 사용\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager._rebuild()\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-activation",
   "metadata": {},
   "source": [
    "# 1. 내부 모듈 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-complexity",
   "metadata": {},
   "source": [
    "- 입력 데이터 → [ batch_size x length ]\n",
    "- Source & Target Embedding → [ batch_size x length x d_emb ]\n",
    "- Positional Encoding 강의 노드에서 구현을 했었죠? 2번의 결과에 더해지므로 shape 변화는 없습니다.\n",
    "- Multi-Head Attention 아래와 같이 여러 개의 서브 모듈들이 존재합니다.\n",
    "    - 1) Split Heads →[ batch_size x length x heads x (d_emb / n_heads) ]\n",
    "    - 2) Masking for Masked Attention\n",
    "    - 3) Scaled Dot Product Attention\n",
    "    - 4) Combine Heads →[ batch_size x length x d_emb ]\n",
    "- Residual Connection\n",
    "- Layer Normalization\n",
    "- Position-wise Feed-Forward Network → [ batch_size x length x d_ff ]\n",
    "- Output Linear Layer → [ batch_size x length x vocab_size ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eligible-defense",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리를 먼저 import \n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "\n",
    "import seaborn # Attention 시각화를 위해 필요!\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlling-maple",
   "metadata": {},
   "source": [
    "### 1) Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "tight-lending",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    return sinusoid_table\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-rebecca",
   "metadata": {},
   "source": [
    "### 2) Multi-Head Attention\n",
    "- Multi-Head Attention은 여러 개의 서브 모듈을 결합하여 완성\n",
    "    - Embedding된 입력을 Head 수로 분할하는 split_heads()\n",
    "    - 분할된 입력으로부터 Attention 값을 구하는 scaled_dot_product_attention()\n",
    "    - 연산이 종료되고 분할된 Head를 다시 하나로 결합시켜주는 combine_heads()  MultiHeadAttention 클래스를 정의\n",
    "- 마스크를 생성하는 함수는 모델을 완성한 후에 구현한다.\n",
    "\n",
    "![트랜스포머_인과관계마스킹ㅡ](https://user-images.githubusercontent.com/70866993/137685024-77d57486-2403-4147-a3df-149ed3c2dc41.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "stylish-refrigerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "            \n",
    "        self.depth = d_model // self.num_heads\n",
    "            \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "            \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):  # 분할된 입력으로부터 Attention 값을 구하기\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "            \n",
    "\n",
    "    def split_heads(self, x):  # Embedding된 입력을 Head 수로 분할\n",
    "        batch_size = x.shape[0]\n",
    "        split_x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):  # 연산이 종료되고 분할된 Head를 다시 하나로 결합\n",
    "        batch_size = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (batch_size, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "        \n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "            \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "    \t\t\t\t        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "                \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-probability",
   "metadata": {},
   "source": [
    "### 3) Position-wise Feed-Forward Network 구현\n",
    "\n",
    "- position 마다, 즉 개별 단어마다 적용되기 때문에 <a href=\"https://pozalabs.github.io/transformer/\" target=\"_blank\">position-wise</a> 입니다.\n",
    "- network는 두 번의 linear transformation과 activation function ReLU로 이루어져 있습니다.\n",
    "$FFN(x) = max(0,xW_1+b_1)+W_2+b_2$\n",
    "    - x 에 linear transformation을 적용한 뒤, $ReLU(max(0,z))$ 를 거쳐 다시 한번 linear transformation을 적용\n",
    "    - 이때 각각의 position마다 같은 parameter $W,b$를 사용하지만, <mark>layer가 달라지면 다른 parameter를 사용</mark>\n",
    "    - kernel size가 1이고 channel이 layer인 convolution을 두 번 수행한 것으로도 위 과정을 이해\n",
    "        - 즉, 한 단어를 Embedding 차원만큼의 채널을 갖는 한칸짜리 이미지라고 취급\n",
    "![feed forward network](https://user-images.githubusercontent.com/70866993/137768669-2717de58-4dd9-4b76-8972-1f51e2b5ad5a.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-width",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.w_1(x)\n",
    "        out = self.w_2(out)\n",
    "            \n",
    "        return out\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-dominican",
   "metadata": {},
   "source": [
    "- d_ff 는 논문의 설명대로라면 2048,  d_model 은 512 ,[ batch x length x d_model ] 의 입력을 받아 w_1 이 2048차원으로 매핑하고 활성함수 ReLU를 적용한 후, 다시 w_2 를 통해 512차원으로 되돌리는 과정까지! 이렇게 쉽게 FFN 완성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-encounter",
   "metadata": {},
   "source": [
    "# 2. 모듈 조립하기\n",
    "- 레이어 수를 원하는 만큼 쌓아 실험을 자유자재로 할 수 있게 모델을 완성\n",
    "    - 방법 :  EncoderLayer, DecoderLayer를 쓸 수 있게 tf.keras.layers.Layer 클래스를 상속받아 레이어 클래스로 정의\n",
    "    \n",
    "        - N = 10\n",
    "        - 10개의 Linear Layer를 한 방에!\n",
    "            - linear_layers = [tf.keras.layers.Dense(30) for _ in range(N)]\n",
    "\n",
    "        - 10개의 Encoder Layer도 한 방에!\n",
    "            - enc_layers = [TransformerEncoderLayer(30) for _ in range(N)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-trade",
   "metadata": {},
   "source": [
    "### 1) Encoder 레이어 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "three-subdivision",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-premium",
   "metadata": {},
   "source": [
    "[ Normalization Layer의 위치에 대한 논의]\n",
    "- 논문에서는 [Input] - [Module] - [Residual] - [Norm] (Module = MHA, FFN)으로 표현\n",
    "- 정작 Official 구현인 구글의 Tensor2Tensor 에서는 [Input] - [Norm] - [Module] - [Residual] 방식을 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-works",
   "metadata": {},
   "source": [
    "### 2) Decoder 레이어 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "funky-character",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, causality_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, padding_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-genre",
   "metadata": {},
   "source": [
    "### 3) Encoder와 Decoder 클래스를 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "defensive-mineral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "purple-paraguay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "                            \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-memorabilia",
   "metadata": {},
   "source": [
    "### 4) Transformer 완성하기\n",
    "- shared 변수를 매개변수로 받아 True 일 경우 Decoder Embedding과 출력층 Linear의 Weight를 공유할 수 있게 하세요! Weight가 공유될 경우 Embedding 값에 sqrt(d_model)을 곱해줘야 하는 것, 잊지 않으셨죠? (참고: tf.keras.layers.Layer.set_weights())\n",
    "\n",
    "- 우리가 정의한 positional_encoding 의 반환값 형태는 [ Length x d_model ] 인데, 이를 더해 줄 Embedding 값 형태가 [ Batch x Length x d_model ] 이라서 연산이 불가능합니다. 연산이 가능하도록 수정하세요! (참고: tf.expand_dims(), np.newaxis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "liked-collection",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared=True):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared = shared\n",
    "\n",
    "        if shared: self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-classics",
   "metadata": {},
   "source": [
    "# 3. 모델 밖의 조력자들"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-principle",
   "metadata": {},
   "source": [
    "###  1)Masking\n",
    "- generate_padding_mask() : Attention을 할 때에 <PAD> 토큰에도 Attention을 주는 것을 방지해 주는 역할\n",
    "    - 한 배치의 데이터에서 <PAD> 토큰으로 이뤄진 부분을 모두 찾아내는 마스크를 생성gksek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "liable-society",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def generate_padding_mask(seq):  # Attention을 할 때에 <PAD> 토큰에도 Attention을 주는 것을 방지해 주는 역할\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "third-correction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 208,
       "width": 426
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch, length = 16, 20\n",
    "src_padding = 5\n",
    "tgt_padding = 15\n",
    "\n",
    "src_pad = tf.zeros(shape=(batch, src_padding))\n",
    "tgt_pad = tf.zeros(shape=(batch, tgt_padding))\n",
    "\n",
    "sample_data = tf.ones(shape=(batch, length))\n",
    "\n",
    "sample_src = tf.concat([sample_data, src_pad], axis=-1)\n",
    "sample_tgt = tf.concat([sample_data, tgt_pad], axis=-1)\n",
    "\n",
    "enc_mask, dec_enc_mask, dec_mask = \\\n",
    "generate_masks(sample_src, sample_tgt)\n",
    "\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax3 = fig.add_subplot(133)\n",
    "\n",
    "ax1.set_title('1) Encoder Mask')\n",
    "ax2.set_title('2) Encoder-Decoder Mask')\n",
    "ax3.set_title('3) Decoder Mask')\n",
    "\n",
    "ax1.imshow(enc_mask[:3, 0, 0].numpy(), cmap='Dark2')\n",
    "ax2.imshow(dec_enc_mask[0, 0].numpy(), cmap='Dark2')\n",
    "ax3.imshow(dec_mask[0, 0].numpy(), cmap='Dark2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-guess",
   "metadata": {},
   "source": [
    "- 첫 번째 마스크는 각 배치 별로 데이터의 꼬리 부분을 Masking 하는 형태\n",
    "- 낯선 부분은 두 번째와 세 번째의 Decoder가 연관된 마스크인데... 이것이 바로 Causality Mask와 Padding Mask를 결합한 형태\n",
    "- 자기 회귀적인 특성을 살리기 위해 Masked Multi-Head Attention에서 인과 관계 마스킹 하는데 인과 관계를 가리는 것도 중요하지만 Decoder 역시 <PAD> 토큰은 피해 가야 하기 때문에 이런 형태의 마스크가 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-communication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서플로우 상에서 잘 구동될수 있도록 LearningRateSchedule 클래스를 상속받아 구현\n",
    "#  가변적인 Learning Rate를 사용하려면 위와 같이 구현\n",
    "\n",
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = LearningRateScheduler(512)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.98, \n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-father",
   "metadata": {},
   "source": [
    "- Optimizer는 논문에 정의된 대로 Adam Optimizer를 사용하며 세부 파라미터도 동일하게 맞춰 줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-publicity",
   "metadata": {},
   "source": [
    "# 프로젝트: 더 멋진 번역기 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-semiconductor",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 1. 데이터 다운로드 (클라우드 유저용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 2. 데이터 정제 및 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/transformer/data'\n",
    "kor_path = data_dir+\"/korean-english-park.train.ko\"\n",
    "eng_path = data_dir+\"/korean-english-park.train.en\"\n",
    "\n",
    "# 데이터 정제 및 토큰화\n",
    "def clean_corpus(kor_path, eng_path):\n",
    "    with open(kor_path, \"r\") as f: kor = f.read().splitlines()\n",
    "    with open(eng_path, \"r\") as f: eng = f.read().splitlines()\n",
    "    assert len(kor) == len(eng)\n",
    "\n",
    "    # [[YOUR CODE]]\n",
    "\n",
    "    return cleaned_corpus\n",
    "\n",
    "cleaned_corpus = clean_corpus(kor_path, eng_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-virtue",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    \n",
    "    # [[YOUR CODE]]\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-wallpaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentencepiece를 활용하여 학습한 tokenizer를 생성합니다.\n",
    "def generate_tokenizer(corpus,\n",
    "                        vocab_size,\n",
    "                        lang=\"ko\",\n",
    "                        pad_id=0,\n",
    "                        bos_id=1,\n",
    "                        eos_id=2,\n",
    "                        unk_id=3):\n",
    "    # [[YOUR CODE]]\n",
    "\n",
    "    return tokenizer\n",
    "    \n",
    "\n",
    "SRC_VOCAB_SIZE = TGT_VOCAB_SIZE = 20000\n",
    "\n",
    "eng_corpus = []\n",
    "kor_corpus = []\n",
    "\n",
    "for pair in cleaned_corpus:\n",
    "    k, e = pair.split(\"\\t\")\n",
    "\n",
    "    kor_corpus.append(preprocess_sentence(k))\n",
    "    eng_corpus.append(preprocess_sentence(e))\n",
    "\n",
    "ko_tokenizer = generate_tokenizer(kor_corpus, SRC_VOCAB_SIZE, \"ko\")\n",
    "en_tokenizer = generate_tokenizer(eng_corpus, TGT_VOCAB_SIZE, \"en\")\n",
    "en_tokenizer.set_encode_extra_options(\"bos:eos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook    # Process 과정을 보기 위해\n",
    "\n",
    "src_corpus = []\n",
    "tgt_corpus = []\n",
    "\n",
    "assert len(kor_corpus) == len(eng_corpus)\n",
    "\n",
    "# 토큰의 길이가 50 이하인 문장만 남깁니다. \n",
    "for idx in tqdm_notebook(range(len(kor_corpus))):\n",
    "    # [[YOUR CODE]]\n",
    "\n",
    "# 패딩처리를 완료하여 학습용 데이터를 완성합니다. \n",
    "enc_train = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='post')\n",
    "dec_train = tf.keras.preprocessing.sequence.pad_sequences(tgt_corpus, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-entrepreneur",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 3. 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-adobe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 4. 훈련하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-jerusalem",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = # [[YOUR CODE]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-annotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = # [[YOUR CODE]]\n",
    "optimizer = # [[YOUR CODE]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    # Masking 되지 않은 입력의 개수로 Scaling하는 과정\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-poker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Step 함수 정의\n",
    "\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    gold = tgt[:, 1:]\n",
    "        \n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "\n",
    "    # 계산된 loss에 tf.GradientTape()를 적용해 학습을 진행합니다.\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "    # 최종적으로 optimizer.apply_gradients()가 사용됩니다. \n",
    "    # [[YOUR CODE]]\n",
    "    \n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-jordan",
   "metadata": {},
   "source": [
    "### 번역 생성에는 아래 소스를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-spelling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention 시각화 함수\n",
    "\n",
    "def visualize_attention(src, tgt, enc_attns, dec_attns, dec_enc_attns):\n",
    "    def draw(data, ax, x=\"auto\", y=\"auto\"):\n",
    "        import seaborn\n",
    "        seaborn.heatmap(data, \n",
    "                        square=True,\n",
    "                        vmin=0.0, vmax=1.0, \n",
    "                        cbar=False, ax=ax,\n",
    "                        xticklabels=x,\n",
    "                        yticklabels=y)\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Encoder Layer\", layer + 1)\n",
    "        for h in range(4):\n",
    "            draw(enc_attns[layer][0, h, :len(src), :len(src)], axs[h], src, src)\n",
    "        plt.show()\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Decoder Self Layer\", layer+1)\n",
    "        for h in range(4):\n",
    "            draw(dec_attns[layer][0, h, :len(tgt), :len(tgt)], axs[h], tgt, tgt)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Decoder Src Layer\", layer+1)\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        for h in range(4):\n",
    "            draw(dec_enc_attns[layer][0, h, :len(tgt), :len(src)], axs[h], src, tgt)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 함수\n",
    "\n",
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "    \n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-testament",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-worry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-palestinian",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
