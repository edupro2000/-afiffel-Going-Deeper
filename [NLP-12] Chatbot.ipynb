{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "greenhouse-romance",
   "metadata": {},
   "source": [
    "## Step 1. 데이터 다운로드\n",
    "- <a href=\"https://github.com/songys/Chatbot_data\" target=\"_blank\"> songys/Chatbot_data </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-magic",
   "metadata": {},
   "source": [
    "#### 심볼릭 링크 디렉토리 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-pearl",
   "metadata": {},
   "source": [
    "- mkdir -p ~/aiffel/transformer_chatbot/data\n",
    "- ln -s ~/data/* ~/aiffel/transformer_chatbot/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "complimentary-purpose",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from konlpy.tag import Mecab\n",
    "from tqdm import tqdm\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "numeric-contamination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [E-15] CHAT-BOT 노드 참고 \n",
    "dataset_filepath = os.getenv('HOME') + '/aiffel/transformer_chatbot/data/ChatbotData .csv'\n",
    "df = pd.read_csv(dataset_filepath)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "colonial-building",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 11823\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플수 :', (len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-insertion",
   "metadata": {},
   "source": [
    "#### 읽어 온 데이터의 질문과 답변을 각각 questions, answers 변수에 나눠서 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "increasing-suggestion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 로드하고 전처리하여 질문을 questions, 답변을 answers에 저장합니다.\n",
    "questions = df['Q']\n",
    "answers = df['A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "clinical-sacrifice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 수 : 11823\n",
      "전체 샘플 수 : 11823\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플 수 :', len(questions))\n",
    "print('전체 샘플 수 :', len(answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-dairy",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 정제\n",
    "- 영문자의 경우, 모두 소문자로 변환합니다.\n",
    "- 영문자와 한글, 숫자, 그리고 주요 특수문자를 제외하곤 정규식을 활용하여 모두 제거합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "occasional-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 노드 8. Seq2seq으로 번역기 만들기, 12노드 참고- 데이터 정제하기\n",
    "\n",
    "def preprocess_sentence(sentence):  #preprocess_sentence :  문장부호와 대소문자등을 정제하는 함수\n",
    "    \n",
    "    sentence = sentence.lower()   # 모든 입력을 소문자로 변환\n",
    "   \n",
    "    sentence = re.sub(r\"[^a-zA-Z가-힣?.!,]+\", \" \", sentence)  # 알파벳, 한글, 문장부호만 남기고 모두 제거\n",
    "    \n",
    "    sentence = '<start> ' + sentence + ' <end>' # 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "rural-heath",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후의 22번째 질문 샘플: 가끔은 혼자인게 좋다\n",
      "전처리 후의 22번째 답변 샘플: 혼자를 즐기세요.\n"
     ]
    }
   ],
   "source": [
    "# 전처리 진행 확인\n",
    "\n",
    "print('전처리 후의 22번째 질문 샘플: {}'.format(questions[13]))\n",
    "print('전처리 후의 22번째 답변 샘플: {}'.format(answers[13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-franklin",
   "metadata": {},
   "source": [
    "## Step 3. 데이터 토큰화\n",
    "- 토큰화에는 KoNLPy의 mecab 클래스를 사용\n",
    "- build_corpus() 함수를 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-shannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "소스 문장 데이터와 타겟 문장 데이터를 입력으로 받습니다.   질문만 하는 나라의 언어를 Source언어 / 답변만 하는 나라의 언어를 Target 언어\n",
    "데이터를 앞서 정의한 preprocess_sentence() 함수로 정제하고, 토큰화합니다.\n",
    "토큰화는 전달받은 토크나이즈 함수를 사용합니다. 이번엔 mecab.morphs 함수를 전달하시면 됩니다.\n",
    "토큰의 개수가 일정 길이 이상인 문장은 데이터에서 제외합니다.\n",
    "중복되는 문장은 데이터에서 제외합니다. 소스 : 타겟 쌍을 비교하지 않고 소스는 소스대로 타겟은 타겟대로 검사합니다. 중복 쌍이 흐트러지지 않도록 유의하세요!\n",
    "구현한 함수를 활용하여 questions 와 answers 를 각각 que_corpus , ans_corpus 에 토큰화하여 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-warning",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-bangladesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에서 만든 전처리 함수에서 소스 문장을 제거하면 타겟 문장이 된다. \n",
    "\n",
    "corpus = []\n",
    "\n",
    "# 모든 문장에 전처리 함수 적용\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "\n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-yesterday",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-saudi",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-management",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-lithuania",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-latvia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-calculator",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-burning",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-alberta",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "coupled-maple",
   "metadata": {},
   "source": [
    "## Step 4. Augmentation  11노드 어휘대체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-switzerland",
   "metadata": {},
   "outputs": [],
   "source": [
    "우리에게 주어진 데이터는 1만 개가량으로 적은 편에 속합니다. 이럴 때에 사용할 수 있는 테크닉을 배웠으니 활용해 봐야겠죠? Lexical Substitution을 실제로 적용해 보도록 하겠습니다.\n",
    "\n",
    "아래 링크를 참고하여 한국어로 사전 훈련된 Embedding 모델을 다운로드합니다. Korean (w) 가 Word2Vec으로 학습한 모델이며 용량도 적당하므로 사이트에서 Korean (w)를 찾아 다운로드하고, ko.bin 파일을 얻으세요!\n",
    "\n",
    "Kyubyong/wordvectors\n",
    "다운로드한 모델을 활용해 데이터를 Augmentation 하세요! 앞서 정의한 lexical_sub() 함수를 참고하면 도움이 많이 될 겁니다.\n",
    "\n",
    "Augmentation된 que_corpus 와 원본 ans_corpus 가 병렬을 이루도록, 이후엔 반대로 원본 que_corpus 와 Augmentation된 ans_corpus 가 병렬을 이루도록 하여 전체 데이터가 원래의 3배가량으로 늘어나도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-reduction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-reading",
   "metadata": {},
   "source": [
    "## Step 5. 데이터 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-terrorism",
   "metadata": {},
   "outputs": [],
   "source": [
    "타겟 데이터인 ans_corpus 에 <start> 토큰과 <end> 토큰이 추가되지 않은 상태이니 이를 먼저 해결한 후 벡터화를 진행합니다. 우리가 구축한 ans_corpus 는 list 형태이기 때문에 아주 쉽게 이를 해결할 수 있답니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = [\"12\", \"시\", \"땡\", \"!\"]\n",
    "\n",
    "print([\"<start>\"] + sample_data + [\"<end>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-agenda",
   "metadata": {},
   "source": [
    "## Step 6. 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-eleven",
   "metadata": {},
   "outputs": [],
   "source": [
    "앞서 번역 모델을 훈련하며 정의한 Transformer 를 그대로 사용하시면 됩니다! 대신 데이터의 크기가 작으니 하이퍼파라미터를 튜닝해야 과적합을 피할 수 있습니다. 모델을 훈련하고 아래 예문에 대한 답변을 생성하세요! 가장 멋진 답변과 모델의 하이퍼파라미터를 제출하시면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-adult",
   "metadata": {},
   "source": [
    "## Step 7. 성능 측정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-ideal",
   "metadata": {},
   "outputs": [],
   "source": [
    "챗봇의 경우, 올바른 대답을 하는지가 중요한 평가 지표입니다. 올바른 답변을 하는지 눈으로 확인할 수 있겠지만, 많은 데이터의 경우는 모든 결과를 확인할 수 없을 것입니다. 주어진 질문에 적절한 답변을 하는지 확인하고, BLEU Score를 계산하는 calculate_bleu() 함수도 적용해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-actor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
