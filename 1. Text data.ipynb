{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "stone-saver",
   "metadata": {},
   "source": [
    "# 텍스트 데이터 다루기\n",
    "- http://www.aistudy.co.kr/linguistics/natural/language_kim.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-kuwait",
   "metadata": {},
   "source": [
    "### 전처리 : 자연어의 노이즈 제거\n",
    "- 불완전한 문장으로 구성된 대화의 경우\n",
    "- 문장의 길이가 너무 길거나 짧은 경우\n",
    "- 채팅 데이터에서 문장 시간 간격이 너무 긴 경우\n",
    "- 바람직하지 않은 문장의 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-shame",
   "metadata": {},
   "source": [
    "### 대표적인 노이즈 유형\n",
    "- (1) 문장부호 : Hi, my name is john.\n",
    "- (2) 대소문자 : First, open the first chapter.*의미를 다르게 받아드릴수 있어서 소문자로 모두 변경이 필요하다.\n",
    "- (3) 특수문자 : He is a ten-year-old boy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-algorithm",
   "metadata": {},
   "source": [
    "#### (1) 문장부호 : Hi, my name is john."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "inner-blond",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi ,  my name is john . \n"
     ]
    }
   ],
   "source": [
    "# (1) 문장부호 : Hi, my name is john. *문장부호 양쪽에 공백을 추가하기\n",
    "\n",
    "def pad_punctuation(sentence, punc):\n",
    "    for p in punc:\n",
    "        sentence = sentence.replace(p, \" \" + p + \" \")  # replace(): 어떤 패턴에 일치하는 일부 또는 모든 부분이 교체된 새로운 문자열을 반환\n",
    "\n",
    "    return sentence\n",
    "\n",
    "sentence = \"Hi, my name is john.\"\n",
    "\n",
    "print(pad_punctuation(sentence, [\".\", \"?\", \"!\", \",\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-terminal",
   "metadata": {},
   "source": [
    "#### (2) 대소문자 : First, open the first chapter.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "driven-durham",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first, open the first chapter.\n"
     ]
    }
   ],
   "source": [
    "# (2) 대소문자 : First, open the first chapter.  *lower\n",
    "\n",
    "sentence = \"First, open the first chapter.\"\n",
    "\n",
    "print(sentence.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "blessed-flower",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST, OPEN THE FIRST CHAPTER.\n"
     ]
    }
   ],
   "source": [
    "# (2) 대소문자 : First, open the first chapter.  *upper\n",
    "\n",
    "sentence = \"First, open the first chapter.\"\n",
    "\n",
    "print(sentence.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-shelter",
   "metadata": {},
   "source": [
    "#### (3) 특수문자 : He is a ten-year-old boy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "lyric-bronze",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is a ten year old boy.\n"
     ]
    }
   ],
   "source": [
    "import re  #정규표현식 사용을 도와주는 패키지 \n",
    "\n",
    "sentence = \"He is a ten-year-old boy.\"\n",
    "sentence = re.sub(\"([^a-zA-Z.,?!])\", \" \", sentence)\n",
    "\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-stadium",
   "metadata": {},
   "source": [
    "-<a href = \"https://hamait.tistory.com/342\" target=\"_blank\"> 정규표현식 정리 참고하기  </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-coalition",
   "metadata": {},
   "source": [
    "### (1)+(2)+(3) 합치기\n",
    "\n",
    "# From The Project Gutenberg\n",
    "# (https://www.gutenberg.org/files/2397/2397-h/2397-h.htm)\n",
    "\n",
    "corpus = \\\n",
    "\"\"\"\n",
    "In the days that followed I learned to spell in this uncomprehending way a great many words, among them pin, hat, cup and a few verbs like sit, stand and walk. \n",
    "But my teacher had been with me several weeks before I understood that everything has a name.\n",
    "One day, we walked down the path to the well-house, attracted by the fragrance of the honeysuckle with which it was covered. \n",
    "Some one was drawing water and my teacher placed my hand under the spout. \n",
    "As the cool stream gushed over one hand she spelled into the other the word water, first slowly, then rapidly. \n",
    "I stood still, my whole attention fixed upon the motions of her fingers. \n",
    "Suddenly I felt a misty consciousness as of something forgotten—a thrill of returning thought; and somehow the mystery of language was revealed to me. \n",
    "I knew then that \"w-a-t-e-r\" meant the wonderful cool something that was flowing over my hand. \n",
    "That living word awakened my soul, gave it light, hope, joy, set it free! \n",
    "There were barriers still, it is true, but barriers that could in time be swept away.\n",
    "\"\"\" \n",
    "\n",
    "def cleaning_text(text, punc, regex):\n",
    "    # 노이즈 유형 (1) 문장부호 공백추가\n",
    "    for p in punc:\n",
    "        text = text.replace(p, \" \" + p + \" \")\n",
    "\n",
    "    # 노이즈 유형 (2), (3) 소문자화 및 특수문자 제거\n",
    "    text = re.sub(regex, \" \", text).lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "print(cleaning_text(corpus, [\".\", \",\", \"!\", \"?\"], \"([^a-zA-Z0-9.,?!\\n])\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-sweden",
   "metadata": {},
   "source": [
    "- 임베딩 레이어(Embedding Layer)를 통해 단어의 분산 표현(distributed representation) 을 구현\n",
    "- 희소 표현방식은 벡터의 각 차원마다 단어의 특정 의미 속성을 대응시키는 방식 *예: 소년: [-1, -1], 소녀: [1, -1]\n",
    "- 희소 표현방식은 워드 벡터끼리는 단어들 간의 의미적 유사도를 계산할수 없는데 코사인 유사도로 보완할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-ballot",
   "metadata": {},
   "source": [
    "## 토큰화(Tokenization) 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-venezuela",
   "metadata": {},
   "source": [
    "###  1) 띄어쓰기 기반 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "executed-realtor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장이 포함하는 Tokens: ['in', 'the', 'days', 'that', 'followed', 'i', 'learned', 'to', 'spell', 'in', 'this', 'uncomprehending', 'way', 'a', 'great', 'many', 'words', ',', 'among', 'them', 'pin', ',', 'hat', ',', 'cup', 'and', 'a', 'few', 'verbs', 'like', 'sit', ',', 'stand', 'and', 'walk', '.', 'but', 'my', 'teacher', 'had', 'been', 'with', 'me', 'several', 'weeks', 'before', 'i', 'understood', 'that', 'everything', 'has', 'a', 'name', '.', 'one', 'day', ',', 'we', 'walked', 'down', 'the', 'path', 'to', 'the', 'well', 'house', ',', 'attracted', 'by', 'the', 'fragrance', 'of', 'the', 'honeysuckle', 'with', 'which', 'it', 'was', 'covered', '.', 'some', 'one', 'was', 'drawing', 'water', 'and', 'my', 'teacher', 'placed', 'my', 'hand', 'under', 'the', 'spout', '.', 'as', 'the', 'cool', 'stream', 'gushed', 'over', 'one', 'hand', 'she', 'spelled', 'into', 'the', 'other', 'the', 'word', 'water', ',', 'first', 'slowly', ',', 'then', 'rapidly', '.', 'i', 'stood', 'still', ',', 'my', 'whole', 'attention', 'fixed', 'upon', 'the', 'motions', 'of', 'her', 'fingers', '.', 'suddenly', 'i', 'felt', 'a', 'misty', 'consciousness', 'as', 'of', 'something', 'forgotten', 'a', 'thrill', 'of', 'returning', 'thought', 'and', 'somehow', 'the', 'mystery', 'of', 'language', 'was', 'revealed', 'to', 'me', '.', 'i', 'knew', 'then', 'that', 'w', 'a', 't', 'e', 'r', 'meant', 'the', 'wonderful', 'cool', 'something', 'that', 'was', 'flowing', 'over', 'my', 'hand', '.', 'that', 'living', 'word', 'awakened', 'my', 'soul', ',', 'gave', 'it', 'light', ',', 'hope', ',', 'joy', ',', 'set', 'it', 'free', '!', 'there', 'were', 'barriers', 'still', ',', 'it', 'is', 'true', ',', 'but', 'barriers', 'that', 'could', 'in', 'time', 'be', 'swept', 'away', '.']\n"
     ]
    }
   ],
   "source": [
    "corpus = \\\n",
    "\"\"\"\n",
    "in the days that followed i learned to spell in this uncomprehending way a great many words ,  among them pin ,  hat ,  cup and a few verbs like sit ,  stand and walk .  \n",
    "but my teacher had been with me several weeks before i understood that everything has a name . \n",
    "one day ,  we walked down the path to the well house ,  attracted by the fragrance of the honeysuckle with which it was covered .  \n",
    "some one was drawing water and my teacher placed my hand under the spout .  \n",
    "as the cool stream gushed over one hand she spelled into the other the word water ,  first slowly ,  then rapidly .  \n",
    "i stood still ,  my whole attention fixed upon the motions of her fingers .  \n",
    "suddenly i felt a misty consciousness as of something forgotten a thrill of returning thought  and somehow the mystery of language was revealed to me .  \n",
    "i knew then that  w a t e r  meant the wonderful cool something that was flowing over my hand .  \n",
    "that living word awakened my soul ,  gave it light ,  hope ,  joy ,  set it free !  \n",
    "there were barriers still ,  it is true ,  but barriers that could in time be swept away . \n",
    "\"\"\"\n",
    "\n",
    "tokens = corpus.split()\n",
    "\n",
    "print(\"문장이 포함하는 Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-prague",
   "metadata": {},
   "source": [
    "### 2) 형태소 기반 토큰화 (품사를 통한..)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-accused",
   "metadata": {},
   "source": [
    "- 사용할 데이터의 특성(띄어쓰기 유무 등)이나 개발 환경(Python, Java)에 따라서 적합한 형태소 분석기를 고려\n",
    "- 연산 속도가 중요하다면 mecab을 최우선으로 고려해야하며, 심지어 분석 품질도 상위권으로 보여짐\n",
    "- 자소 분리나 오탈자에 대해서도 어느 정도 분석 품질이 보장되야 한다면 KOMORAN+mecab,꼬꼬마) 사용을 고려\n",
    "- 한나눔과 khaiii(카카오)는 일부 케이스에 대한 분석 품질, 꼬꼬마는 분석 시간에서 약간 아쉬운 점이 보임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-calendar",
   "metadata": {},
   "source": [
    "#### 1) konlpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "rental-decimal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  konlpy 및 Mecab의 설치 여부를 확인\n",
    "\n",
    "from konlpy.tag import Hannanum,Kkma,Komoran,Mecab,Okt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-parking",
   "metadata": {},
   "source": [
    "[설치 진행]\n",
    "\n",
    "- pip install konlpy\n",
    "- cd ~/aiffel/text_preprocess\n",
    "- git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
    "- cd Mecab-ko-for-Google-Colab\n",
    "- bash install_mecab-ko_on_colab190912.sh\n",
    "\n",
    "[자바 기반 konlpy의 JVMNotFoundException 오류가 발생시]\n",
    "-  sudo apt update\n",
    "-  sudo apt install default-jre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-snowboard",
   "metadata": {},
   "source": [
    "#### 2) 카카오에서 제공하는 Khaiii 형태소 분석기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "italic-moment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 카카오에서 제공하는 Khaiii 형태소 분석기  *윈도우 미지원\n",
    "\n",
    "# import khaiii\n",
    "\n",
    "# api = khaiii.KhaiiiApi()\n",
    "# api.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-petroleum",
   "metadata": {},
   "source": [
    "- 윈도우 미지원 대비  : https://yj-79.tistory.com/11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "productive-debut",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Khaiii를 konlpy tokenizer처럼 사용하기 위한 wrapper class입니다. \n",
    "\n",
    "# class Khaiii():\n",
    "#     def pos(self, phrase, flatten=True, join=False):\n",
    "#         \"\"\"POS tagger.\n",
    "\n",
    "#         :param flatten: If False, preserves eojeols.\n",
    "#         :param join: If True, returns joined sets of morph and tag.\n",
    "\n",
    "#         \"\"\"\n",
    "#         sentences = phrase.split('\\n')\n",
    "#         morphemes = []\n",
    "#         if not sentences:\n",
    "#             return morphemes\n",
    "\n",
    "#         for sentence in sentences:\n",
    "#             for word in api.analyze(sentence):\n",
    "#                 result = [(m.lex, m.tag) for m in word.morphs]\n",
    "#                 if join:\n",
    "#                     result = ['{}/{}'.format(m.lex, m.tag) for m in word.morphs]\n",
    "\n",
    "#                 morphemes.append(result)\n",
    "\n",
    "#         if flatten:\n",
    "#             return sum(morphemes, [])\n",
    "\n",
    "#         return morphemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "subtle-violence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 6개의 형태소 분석기의 문장 해석 살펴보기\n",
    "\n",
    "# tokenizer_list = [Hannanum(),Kkma(),Komoran(),Mecab(),Okt(),Khaiii()]\n",
    "\n",
    "# kor_text = '코로나바이러스는 2019년 12월 중국 우한에서 처음 발생한 뒤 전 세계로 확산된, 새로운 유형의 호흡기 감염 질환입니다.'\n",
    "\n",
    "# for tokenizer in tokenizer_list:\n",
    "#     print('[{}] \\n{}'.format(tokenizer.__class__.__name__, tokenizer.pos(kor_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-witness",
   "metadata": {},
   "source": [
    "#### Wordpiece Model *SentencePiece 라이브러리를 통해 고성능의 BPE를 사용\n",
    "- 한 단어를 여러 개의 Subword의 집합으로 보는 방법이 WPM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cleared-bearing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Step 1\n",
      "다음 문자 쌍을 치환: es\n",
      "변환된 Vocab:\n",
      " {'l o w ': 5, 'l o w e r ': 2, 'n e w es t ': 6, 'w i d es t ': 3} \n",
      "\n",
      ">> Step 2\n",
      "다음 문자 쌍을 치환: est\n",
      "변환된 Vocab:\n",
      " {'l o w ': 5, 'l o w e r ': 2, 'n e w est ': 6, 'w i d est ': 3} \n",
      "\n",
      ">> Step 3\n",
      "다음 문자 쌍을 치환: lo\n",
      "변환된 Vocab:\n",
      " {'lo w ': 5, 'lo w e r ': 2, 'n e w est ': 6, 'w i d est ': 3} \n",
      "\n",
      ">> Step 4\n",
      "다음 문자 쌍을 치환: low\n",
      "변환된 Vocab:\n",
      " {'low ': 5, 'low e r ': 2, 'n e w est ': 6, 'w i d est ': 3} \n",
      "\n",
      ">> Step 5\n",
      "다음 문자 쌍을 치환: ne\n",
      "변환된 Vocab:\n",
      " {'low ': 5, 'low e r ': 2, 'ne w est ': 6, 'w i d est ': 3} \n",
      "\n",
      "Merge Vocab: ['es', 'est', 'lo', 'low', 'ne']\n"
     ]
    }
   ],
   "source": [
    "#### 토큰화 적용 논문 및 코드(  https://arxiv.org/pdf/1508.07909.pdf)\n",
    "\n",
    "import re, collections\n",
    "\n",
    "# 임의의 데이터에 포함된 단어들입니다.\n",
    "# 우측의 정수는 임의의 데이터에 해당 단어가 포함된 빈도수입니다.\n",
    "vocab = {\n",
    "    'l o w '      : 5,\n",
    "    'l o w e r '  : 2,\n",
    "    'n e w e s t ': 6,\n",
    "    'w i d e s t ': 3\n",
    "}\n",
    "\n",
    "num_merges = 5\n",
    "\n",
    "def get_stats(vocab):\n",
    "    \"\"\"\n",
    "    단어 사전을 불러와\n",
    "    단어는 공백 단위로 쪼개어 문자 list를 만들고\n",
    "    빈도수와 쌍을 이루게 합니다. (symbols)\n",
    "    \"\"\"\n",
    "    pairs = collections.defaultdict(int)\n",
    "    \n",
    "    for word, freq in vocab.items():\n",
    "\n",
    "        for i in range(len(symbols) - 1):             # 모든 symbols를 확인하여 \n",
    "            pairs[symbols[i], symbols[i + 1]] += freq  # 문자 쌍의 빈도수를 저장합니다. \n",
    "        \n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    \n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "        \n",
    "    return v_out, pair[0] + pair[1]\n",
    "\n",
    "token_vocab = []\n",
    "\n",
    "for i in range(num_merges):\n",
    "    print(\">> Step {0}\".format(i + 1))\n",
    "    \n",
    "    pairs = get_stats(vocab)\n",
    "    best = max(pairs, key=pairs.get)  # 가장 많은 빈도수를 가진 문자 쌍을 반환합니다.\n",
    "    vocab, merge_tok = merge_vocab(best, vocab)\n",
    "    print(\"다음 문자 쌍을 치환:\", merge_tok)\n",
    "    print(\"변환된 Vocab:\\n\", vocab, \"\\n\")\n",
    "    \n",
    "    token_vocab.append(merge_tok)\n",
    "    \n",
    "print(\"Merge Vocab:\", token_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-relative",
   "metadata": {},
   "source": [
    "Wordpiece Model(WPM) *비공개\n",
    "- 구글에서 BPE를 변형해 제안한 알고리즘이 바로 WPM\n",
    "- 공백 복원을 위해 단어의 시작 부분에 언더바 _ 를 추가\n",
    "- 빈도수 기반이 아닌 가능도(Likelihood,확률 분포에 나왔을지에 대한 확률, 즉 확률의 확률)를 증가시키는 방향으로 문자 쌍을 합치기\n",
    "- 조사, 어미 등의 활용이 많고 복잡한 한국어 같은 모델의 토크나이저로 WPM이 좋은 대안"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-sensitivity",
   "metadata": {},
   "source": [
    "## 워드 임베딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-samoa",
   "metadata": {},
   "source": [
    "- 한국어 단어에 대해서 벡터 연산을 해 볼수 있는 사이트 http://w.elnn.kr/search/\n",
    "- Word2Vec과 FastText는 워드의 벡터값이 동일하다. 예)사과 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-tomorrow",
   "metadata": {},
   "source": [
    "### 1) 워드투벡터(Word2Vec) https://wikidocs.net/22660\n",
    "- 은닉층이 1개라 딥러닝이 아닌 Shallow Neural Network라고 불린다.\n",
    "- 성능이 좋은 Negative Sampling + Skip-gram 을 합쳐서 SGNS 라는 말로 Word2Vec을 대신하기도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-easter",
   "metadata": {},
   "source": [
    "- 희소표현 : 벡터 또는 행렬(matrix)의 값이 대부분이 0으로 표현되는 방법 *원핫인코딩\n",
    "- 분산표현 : 희소표현은 단어간 유사성을 표현할수 없는 단점이 있고 이를 위한 대안으로 단어의'의미'를 다차원 공간에 벡터화 하는 방법\n",
    "    - 비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다 의 가정\n",
    "- CBOW(Continuous Bag of Words) : 주변단어에서 중심단어 예측\n",
    "    -  CBOW는 주변 단어로 중심 단어를 더 정확히 맞추기 위해 계속해서 이 W와 W'를 학습해가는 구조 \n",
    "- Skip-Gram  : 중심단어 기준으로 주변 단어 예측\n",
    "    - 중심단어에 대해 주변단어 예측으로 투사층에서 벡터들의 평균을 구하는 과정은 없다. \n",
    "    - 성능비교 : CBOW < Skip-gram\n",
    "- 성능 개선을 위해 두가지 제안 Hierarchical Softmax, Negative Sampling\n",
    "    - Hierarchical Softmax : 등장횟수 확인 > 이진트리 구성 >높은 빈도의 단어를 >하위 노드>하위노드로 단어를 저장하는 방식으로 구성\n",
    "    - Negative Sampling : 기준 단어마다 실제로 등장했던(positive), 등자하지 않았던(negative) 예시를 구성하여 이진로지스틱분류기를 학습\n",
    "    - 성능비교 : Hierarchical Softmax < Negative Sampling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-fifty",
   "metadata": {},
   "source": [
    "[용어정리]\n",
    "- 윈도우(window) : 중심 단어를 예측하기 위해서 앞, 뒤로 몇 개의 단어를 볼지를 결정했다면 이 범위\n",
    "- 슬라이딩 윈도우(sliding window) : 윈도우를 계속 움직여서 주변 단어와 중심 단어 선택을 바꿔가며 학습을 위한 데이터 셋을 만드는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-melissa",
   "metadata": {},
   "source": [
    "### 2) FastText\n",
    "- FastText는 한 단어를 n-gram의 집합이라고 보고 단어를 쪼개어 각 n-gram에 할당된 Embedding의 평균값을 사용하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-biography",
   "metadata": {},
   "source": [
    "### 3) ELMo - the 1st Contextualized Word Embedding\n",
    "- 그 단어가 놓인 주변 단어 배치의 맥락이 함께 고려되는 Word Embedding\n",
    "- 전이학습, 기학습된 언어 모델을 이용해 어휘 임베딩을 생성하는 방법\n",
    "- https://brunch.co.kr/@learning/12- \n",
    "- 기존 어휘 임베딩(입력 토큰의 word vector)+순방향 및 역방향 LSTM의 hidden state vector 를 합쳐서 얻는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-chrome",
   "metadata": {},
   "source": [
    "![ELMO](https://user-images.githubusercontent.com/70866993/134871467-66f098fb-f77b-4ff8-8b59-c27abb30fcba.JPG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-tomorrow",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-istanbul",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-singing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-anger",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-creator",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
