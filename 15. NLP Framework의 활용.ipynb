{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dress-ethics",
   "metadata": {},
   "source": [
    "# 15-2. 다양한 NLP Framework의 출현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-penalty",
   "metadata": {},
   "source": [
    "### General Framework for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-express",
   "metadata": {},
   "source": [
    "- 이번 스텝의 내용은 Top NLP Libraries to Use 2020에 정리된 내용을 바탕으로 작성\n",
    "\n",
    "- General Framework for NLP\n",
    "    - 여기서 소개할 framework들은 NLP 문제를 가장 일반적으로 해결할 수 있는 통합적인 프레임워크를 목표로 설계된 것들입니다. \n",
    "    - 대표적으로는 AllenNLP, Fairseq, Fast.ai가 있으며, Google의 tensor2tensor 프로젝트도 같은 범주로 생각할 수 있습니다.\n",
    "\n",
    "- 🔶 AllenNLP\n",
    "제공자 : Allen AI Institute\n",
    "Website : https://allennlp.org/\n",
    "Github : https://github.com/allenai/allennlp\n",
    "Backend : PyTorch\n",
    "\n",
    "2018년 초반에 Contextual Word Embedding의 대표적인 모델인 ELMO를 발표하면서 유명해진 Allen Institute에서 만든 NLP framework입니다. 당시 ELMO는 GLUE Benchmark Test와 같이 10가지나 되는 다양한 태스크로 구성된 데이터셋을 하나의 모델을 finetune하는 것만으로도 기존의 State-of-the-art 기록을 경신하는 성능을 보여주고자 하였습니다. 그랬기에 하나의 모델로 다양한 태스크를 손쉽게 처리할 수 있는 유연한 프로젝트를 구성해야 했고, 이를 확장하면서 자연스럽게 NLP framework로 발전하게 되었습니다. 이후 AllenNLP는 Glue dataset의 baseline 프로젝트 Starting Baseline를 제공하기도 했습니다.\n",
    "![15-AllenNLP](https://user-images.githubusercontent.com/70866993/141037573-7dd0e6b8-a78b-45cc-9eeb-86360620a47a.png)\n",
    "             [출처 : AllenNLP Guide(https://guide.allennlp.org/building-your-model#1)]\n",
    "\n",
    "태스크와 모델을 분리해서, 한 가지 모델로 다양한 태스크를 처리하거나 하나의 태스크를 다양한 모델로 처리할 수 있도록 하는 설계는 AllenNLP가 처음 시도한 것은 물론 아닙니다만, ELMO와 같은 pretrained model의 성공을 바탕으로 NLP framework를 완성해 나가려는 AllenNLP의 시도는 이후 많은 아이디어를 제공하였습니다. AllenNLP는 현재는 ELMO 이외에도 BERT 등 다양한 모델의 활용이 가능합니다.\n",
    "\n",
    "단, AllenNLP는 PyTorch 기반으로 설계되었으며 모델이 torch.nn.Module을 상속받는 구조로 설계되었습니다. Tensorflow나 Keras 기반으로 AllenNLP를 활용하는 것은 어렵습니다.\n",
    "\n",
    "- 🔶 Fairseq\n",
    "제공자 : Facebook AI Research\n",
    "Website : https://fairseq.readthedocs.io/en/latest\n",
    "Github : https://github.com/pytorch/fairseq\n",
    "Backend : PyTorch\n",
    "Fairseq는 꾸준히 NLP 연구성과를 내고 있는 Facebook AI Research의 NLP Framework입니다. 비단 자연어처리에만 국한된 것이 아니라 이름에서도 알 수 있듯이 CNN, LSTM 등 전통적인 모델로부터, 음성인식/합성 등 sequential한 데이터를 다루는 분야를 두루 다루는 다양한 pretrained model을 함께 제공하고 있습니다. 역시 Facebook의 framework답게 PyTorch 기반으로 설계되었습니다.\n",
    "\n",
    "- 🔶 Fast.ai\n",
    "제공자 : fast.ai\n",
    "Website : http://docs.fast.ai/\n",
    "Github : https://github.com/fastai/fastai\n",
    "Backend : PyTorch\n",
    "fast.ai는 이름에 걸맞게, 빠르게 배우고 쉽게 모델을 구성할 수 있도록 하이레벨 API와 Application 블록까지 손쉽게 사용할 수 있도록 구성되어 있습니다. 비단 NLP 분야 뿐 아니라 다양한 분야로 확장 가능합니다. 역시 PyTorch 기반으로 설계되었습니다.\n",
    "\n",
    "![15-Fast](https://user-images.githubusercontent.com/70866993/141037664-0ff451be-2e96-4c30-834f-912021d835d8.png)\n",
    "                                  [출처 : fast.ai (https://github.com/fastai/fastai)]\n",
    "\n",
    "- 🔶 tensor2tensor\n",
    "제공자 : Google Brain\n",
    "Github : https://github.com/tensorflow/tensor2tensor (deprecated)\n",
    "New Github : https://github.com/google/trax\n",
    "Backend : Tensorflow\n",
    "Google Brain에서 2017년에 transformer 논문을 발표하면서 그 구현체로 함께 공유했던 프로젝트가 바로 tensor2tensor였습니다. 이 프로젝트 역시 'Attention is all you need'라는 논문의 제목처럼, transformer를 중심으로 다양한 태스크와 다양한 모델을 하나의 framework에 통합하려는 시도를 하였습니다. 이후 2019년부터 Google은 Tensorflow V2 기반으로 pretrained model의 지원을 강화한 trax라는 프로젝트를 생성하면서, 2020년도부터는 tensor2tensor의 개발을 중단하고 관련 기능을 trax로 통합이관하였습니다.\n",
    "\n",
    "Tensorflow 기반의 NLP framework이 상대적으로 드문 가운데, Tensorflow 기반의 NLP 연구개발을 진행한다면 주목해 볼 만할 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-functionality",
   "metadata": {},
   "source": [
    "#### Preprocessing Libraries\n",
    "- 전통적으로 사용되었던 NLP 분야의 전처리 관련 framework들입니다. \n",
    "- 위에서 소개한 framework들처럼 전처리-모델링-태스크 훈련/평가를 통합적으로 설계하여 NLP 태스크를 제너럴하게 수행하게 설계한 것이 아니라 tokenization, tagging, parsing 등 <b>특정 전처리 작업을 위해 설계된 라이브러리에 가깝습니다. </b>\n",
    "- 여기에는 아래 예시로 든 Spacy, NLTK, TorchText 등이 있습니다. 한국어인 경우 KoNLPy 라이브러리도 동일한 역할을 한다고 볼 수 있겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-butter",
   "metadata": {},
   "source": [
    "🔶 Spacy\n",
    "Website : https://spacy.io/\n",
    "Github : https://github.com/explosion/spaCy\n",
    "🔶 NLTK\n",
    "Website : https://www.nltk.org/\n",
    "Github : https://github.com/nltk/nltk\n",
    "🔶 TorchText\n",
    "Website : https://torchtext.readthedocs.io/en/latest/\n",
    "Github : https://github.com/pytorch/text\n",
    "🔶 KoNLPy\n",
    "Website : https://konlpy.org/en/latest/\n",
    "Github : https://github.com/konlpy/konlpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-minority",
   "metadata": {},
   "source": [
    "#### Transformer-based Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-scene",
   "metadata": {},
   "source": [
    "🔶 Huggingface transformers\n",
    "- 제공자 : Huggingface.co\n",
    "- Website : https://huggingface.co/transformers/\n",
    "- Github : https://github.com/huggingface/transformers\n",
    "- Backend : PyTorch and Tensorflow\n",
    "- 여기에는 현재 가장 주목받고 있는 NLP Framework인 Huggingface transformers가 있습니다. 사실 Huggingface의 transformers 라이브러리의 최근 모습은 이미 아주 general한 NLP framework의 모습을 충분히 가지고 있습니다. \n",
    "- 하지만 초기에는 BERT 등 다양한 transformer 기반의 pretrained model을 사용하기 위한 PyTorch 기반의 wrapper 형태로 시작되었습니다. 그래서 전통적인 모델까지 포괄하려고 했던 이전의 general NLP Framework 들에 비해, Huggingface의 transformers는 pretrained model 활용을 주로 지원하며, tokenizer 등 전처리 부분도 pretrained model들이 주로 사용하는 Subword tokenizer 기법에 집중되어 있는 특징이 있습니다.\n",
    "\n",
    "이후 다음 스텝부터는 Huggingface의 transformers에 대해 집중적으로 살펴보겠습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepting-workstation",
   "metadata": {},
   "source": [
    "#### Huggingface transformers 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-revolution",
   "metadata": {},
   "source": [
    "- (1) 광범위하고 신속한 NLP 모델 지원\n",
    "Huggingface는 많은 사람들이 최신 NLP 모델들을 더욱 손쉽게 사용하는 것을 목표로 만들기 시작했다고 합니다. 그래서 그런지 새로운 논문들이 발표될 때마다, 본인들의 framework에 흡수시키고 있습니다. 또한, pretrained model을 제공하고, dataset과 tokenizer를 더욱 쉽게 이용할 수 있도록 framework화시키고 있는 행보도 보이고 있습니다. 다른 framework들도 이런 작업을 하지 않는 것은 아니지만, Huggingface의 지원 범위가 가장 광범위하고, 최신 논문을 지원하는 속도도 빠릅니다.\n",
    "\n",
    "- (2) PyTorch와 Tensorflow 모두에서 사용 가능\n",
    "transformers는 기본적으로 PyTorch를 기반으로 만들어져있습니다. 많은 utility가 PyTorch 위주로 작성이 되어있긴 하지만, 최근에는 Tensorflow로도 학습하고 사용할 수 있게끔 계속해서 framework를 확장하고 있는 중입니다. 이렇듯 Huggingface transformers를 바탕으로 Tensorflow와 PyTorch라는 Backend의 한계를 뛰어넘어 어떤 환경에든 쉽게 적용 가능한 표준 framework의 지위를 다져가고 있습니다.\n",
    "\n",
    "- (3) 잘 설계된 framework 구조\n",
    "HuggingFace의 목표처럼 이 framework는 쉽고 빠르게 어떠한 환경에서도 NLP모델을 사용할 수 있도록 끊임없이 변화하고 있습니다. 이것은 또한 사용하기 쉽고 직관적일뿐더러 모델이나 태스크, 데이터셋이 달라지더라도 동일한 형태로 사용 가능하도록 잘 추상화되고 모듈화된 API 설계가 있기 때문에 가능한 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-tuesday",
   "metadata": {},
   "source": [
    "## 15-3. Huggingface transformers 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-simulation",
   "metadata": {},
   "source": [
    "-  pip install transformers  #클라우드 사용자 미설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "universal-turtle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1260602ad4ac45638cd7181188ebb0a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a29b0a91c14cffb85c0d5066b21d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba4e0366faf40cd95c47a223f95f1a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2c3a1b5f0b84e7c8f954d0d2e970c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9978193640708923}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('sentiment-analysis', framework='tf')\n",
    "classifier('We are very happy to include pipeline into the transformers repository.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-gathering",
   "metadata": {},
   "source": [
    "#### transformers는 위와 같은 흐름에 맞추어 설계\n",
    "- 1) Task를 정의하고 그에 맞게 dataset을 가공시킵니다. Processors\n",
    "    - 텍스트 데이터를 전처리할 수 있는 Tokenizer\n",
    "- 2) 다양한 model을 선택하고  이를 만듭니다. \n",
    "    - optimizer와 학습 schedule(warm up 등)을 관리할 수 있는 Optimization\n",
    "- 3) model에 데이터들을 태워서 학습(Trainer)을 시키고, 이를 통해 나온 \n",
    "- 4) weight와 설정(config)들을 저장합니다. 저장한 model의 checkpoint는 \n",
    "     - weight와 tokenizer, model을 쉽게 불러올 수 있도록 각종 설정을 저장하는 Config \n",
    "- 5) 배포하거나, evaluation을 할 때 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-hazard",
   "metadata": {},
   "source": [
    "## 15-4. Huggingface transformers (1) Model\n",
    "- 기본적으로 모델들은 PretrainedModel 클래스를 상속받고 있습니다. \n",
    "- PretrainedModel 클래스는 학습된 모델을 불러오고, 다운로드하고, 저장하는 등 모델 전반에 걸쳐 적용되는 메소드들을 가지고 있습니다. \n",
    "- 이런 상속 구조를 가지고 있기 때문에, 실제로 사용할 모델이 BERT이건, GPT이건 상관없이 모델을 불러오고 다운로드/저장하는 등의 작업에 활용하는 메소드는 부모 클래스의 것을 동일하게 활용할 수 있게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-captain",
   "metadata": {},
   "source": [
    "### 모델 불러오기 방법1. \n",
    "- 첫 번째로는 task에 적합한 모델을 직접 선택하여 import하고, 불러오는 방식이 있습니다. 모델을 로드할 때는 from_pretrained라는 메소드를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "comparable-canadian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf7413b457354b29821489364c977bb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae09057282db4317baa745d10d83b7f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/527M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForPreTraining.\n",
      "\n",
      "All the layers of TFBertForPreTraining were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForPreTraining for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.modeling_tf_bert.TFBertForPreTraining'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForPreTraining\n",
    "model = TFBertForPreTraining.from_pretrained('bert-base-cased') # 모델 id bert-base-cased\n",
    "\n",
    "print(model.__class__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-retirement",
   "metadata": {},
   "source": [
    "### 모델 불러오기 방법2. \n",
    "- 두 번째 방법은, AutoModel을 이용하는 것입니다.\n",
    "- 모델에 관한 정보를 처음부터 명시하지 않아도 되어 조금 유용하게 사용하실 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "annoying-recruitment",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.modeling_tf_bert.TFBertModel'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModel\n",
    "model = TFAutoModel.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "print(model.__class__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-ballet",
   "metadata": {},
   "source": [
    "## 15-5. Huggingface transformers (2) Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-madrid",
   "metadata": {},
   "source": [
    "#### 모델에 넣을 input을 만들어 줄 차례\n",
    "- transformers는 다양한 tokenizer를 각 모델에 맞추어 이미 구비\n",
    "- tokenizer 또한 직접 명시하여 내가 사용할 것을 지정해 주거나, AutoTokenizer를 사용하여 이미 구비된 model에 알맞은 tokenizer를 자동으로 불러올 수도 있습니다.\n",
    "- 이때 <mark>유의할 점은, model을 사용할 때 명시했던 것과 동일한 ID로 tokenizer를 생성해야 한다는 점입니다.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "extensive-federal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca519e6d53d4ac7b1812fa5d5e5443a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "engaging-guarantee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db7ae2d1fc44e3c835402101feec277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-minutes",
   "metadata": {},
   "source": [
    "#### - BERT의 tokenizer이기 때문에 인코딩이 된 input_ids 뿐만 아니라, token_type_ids와 attention_mask까지 모두 생성된 input 객체를 받아볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "small-breathing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1188, 1110, 5960, 1111, 170, 11093, 1883, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer(\"This is Test for aiffel\")\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-steel",
   "metadata": {},
   "source": [
    "#### - tokenizer는 batch 단위로 input을 받을 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "owned-pizza",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 8667, 146, 112, 182, 170, 1423, 5650, 102], [101, 1262, 1330, 5650, 102], [101, 1262, 1103, 1304, 1304, 1314, 1141, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "batch_sentences = [\"Hello I'm a single sentence\",\n",
    "                    \"And another sentence\",\n",
    "                    \"And the very very last one\"]\n",
    "\n",
    "encoded_batch = tokenizer(batch_sentences)\n",
    "print(encoded_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-stewart",
   "metadata": {},
   "source": [
    "#### return_tensors : 프레임워크를 사용하는가(Tensorflow 또는 PyTorch)에 따라 input 타입을 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "incorporate-guess",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(3, 9), dtype=int32, numpy=\n",
      "array([[ 101, 8667,  146,  112,  182,  170, 1423, 5650,  102],\n",
      "       [ 101, 1262, 1330, 5650,  102,    0,    0,    0,    0],\n",
      "       [ 101, 1262, 1103, 1304, 1304, 1314, 1141,  102,    0]],\n",
      "      dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(3, 9), dtype=int32, numpy=\n",
      "array([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(3, 9), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 0]], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "batch = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"tf\") \n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-cathedral",
   "metadata": {},
   "source": [
    "## 15-6. Huggingface transformers (3) Processor\n",
    "- 아래는 Processor 중 Sequence Classification 태스크를 위한 추상 클래스인 DataProcessor의 코드 예제\n",
    "- processor는 raw data를 가공하여 model에 태울 수 있는 형태를 만들어주는 작업을 해주는 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "million-hebrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"sequence classification을 위해 data를 처리하는 기본 processor\"\"\"\n",
    "\n",
    "    def get_example_from_tensor_dict(self, tensor_dict):\n",
    "        \"\"\"\n",
    "        tensor dict에서 example을 가져오는 메소드\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"train data에서 InputExample 클래스를 가지고 있는 것들을 모으는 메소드\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"dev data(validation data)에서 InputExample 클래스를 가지고 있는 것들을 모으는 메소드\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"test data에서 InputExample 클래스를 가지고 있는 것들을 모으는 메소드\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"data set에 사용되는 라벨들을 리턴하는 메소드\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def tfds_map(self, example):\n",
    "        \"\"\"\n",
    "        tfds(tensorflow-datasets)에서 불러온 데이터를 DataProcessor에 알맞게 가공해주는 메소드\n",
    "        \"\"\"\n",
    "        if len(self.get_labels()) > 1:\n",
    "            example.label = self.get_labels()[int(example.label)]\n",
    "        return example\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"tab으로 구분된 .tsv파일을 읽어들이는 클래스 메소드\"\"\"\n",
    "        with open(input_file, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "            return list(csv.reader(f, delimiter=\"\\t\", quotechar=quotechar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-complement",
   "metadata": {},
   "source": [
    "## 15-7. Huggingface transformers (4) Config\n",
    "- hugging face의 pretrained model을 그대로 사용하게 되면 자동으로 config파일이 로드되어 명시할 필요가 없지만, <B> 설정을 변경하고 싶거나 나만의 모델을 학습시킬 때에는 config파일을 직접 불러와야 합니다.</B>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-window",
   "metadata": {},
   "source": [
    "#### BertConfig 방법1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cosmetic-watson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.configuration_bert.BertConfig'>\n",
      "BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.2.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig\n",
    "\n",
    "config = BertConfig.from_pretrained(\"bert-base-cased\")\n",
    "print(config.__class__)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-valuation",
   "metadata": {},
   "source": [
    "#### AutoConfig 방법2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "deluxe-adventure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.configuration_bert.BertConfig'>\n",
      "BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.2.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"bert-base-cased\")\n",
    "print(config.__class__)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-columbia",
   "metadata": {},
   "source": [
    "####  만약 모델을 이미 생성했다면 model.config으로 가져올 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "diagnostic-teacher",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForPreTraining.\n",
      "\n",
      "All the layers of TFBertForPreTraining were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForPreTraining for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.configuration_bert.BertConfig'>\n",
      "BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.2.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = TFBertForPreTraining.from_pretrained('bert-base-cased')\n",
    "\n",
    "config = model.config\n",
    "print(config.__class__)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-catholic",
   "metadata": {},
   "source": [
    "## 15-8. Huggingface transformers (5) Trainer\n",
    "- trainer는 모델을 학습시키기 위한 클래스\n",
    "- tensorflow의 경우 tf.keras.model API를 이용하여서도 Huggingface를 통해 불러온 모델을 활용해 학습이나 테스트를 진행 가능\n",
    "- TFTrainer를 이용할 경우에는 TrainingArguments 를 통해 Huggingface 프레임워크에서 제공하는 기능들을 통합적으로 커스터마이징하여 모델을 학습시킬 수 있다는 장점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "seven-solid",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForPreTraining.\n",
      "\n",
      "All the layers of TFBertForPreTraining were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForPreTraining for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7ff96fb42440>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7ff96fb42440>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Results=====\n",
      "TFBertForPreTrainingOutput(loss=None, prediction_logits=array([[[ -7.402721 ,  -7.362662 ,  -7.4500194, ...,  -6.195526 ,\n",
      "          -5.8948097,  -6.3672643],\n",
      "        [ -7.8287287,  -8.058227 ,  -7.8642054, ...,  -6.419407 ,\n",
      "          -6.3024297,  -6.7624664],\n",
      "        [-11.549938 , -11.551907 , -11.484703 , ...,  -8.114803 ,\n",
      "          -8.314196 ,  -9.444446 ],\n",
      "        ...,\n",
      "        [ -3.2660685,  -3.7416432,  -2.5797977, ...,  -4.0110044,\n",
      "          -2.4964385,  -3.0753932],\n",
      "        [-12.231962 , -12.027049 , -11.797822 , ...,  -8.838849 ,\n",
      "          -9.091648 , -10.497255 ],\n",
      "        [-10.639943 , -11.074335 , -11.036109 , ...,  -8.148462 ,\n",
      "          -9.585203 , -10.671506 ]]], dtype=float32), seq_relationship_logits=array([[ 1.6309197 , -0.71684647]], dtype=float32), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForPreTraining, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "model = TFAutoModelForPreTraining.from_pretrained('bert-base-cased')\n",
    "\n",
    "sentence = \"Hello, This is test for bert TFmodel.\"\n",
    "\n",
    "input_ids = tf.constant(tokenizer.encode(sentence, add_special_tokens=True))[None, :]  # Batch size 1\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss)\n",
    "pred = model.predict(input_ids)\n",
    "\n",
    "print(\"=====Results=====\")\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-guyana",
   "metadata": {},
   "source": [
    "#### TFTrainer를 사용할 경우에는 학습에 필요한 arguments을 TFTrainingArguments을 통해서 정의해 주어야 한다.\n",
    "- 아래는 TFTrainer를 사용하여 Huggingface 모델의 학습이 이루어지는 아주 간단한 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "broadband-virus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3201b0b7cff1422d96261c1a1d28c06c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8706d5ac1b4a4e6eafa9d3ac34560127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/536M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2996dbc1ff86442cbd3c388e059d70ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aed512fb7be4da2893f656f6060d8d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "from typing import Dict, Optional\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from transformers import (\n",
    "    TFAutoModelForSequenceClassification,\n",
    "    TFTrainer,\n",
    "    TFTrainingArguments,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    glue_convert_examples_to_features,\n",
    ")\n",
    "\n",
    "# TFTrainingArguments 정의\n",
    "training_args = TFTrainingArguments(\n",
    "    output_dir='./results',              # output이 저장될 경로\n",
    "    num_train_epochs=1,              # train 시킬 총 epochs\n",
    "    per_device_train_batch_size=16,  # 각 device 당 batch size\n",
    "    per_device_eval_batch_size=64,   # evaluation 시에 batch size\n",
    "    warmup_steps=500,                # learning rate scheduler에 따른 warmup_step 설정\n",
    "    weight_decay=0.01,                 # weight decay\n",
    "    logging_dir='./logs',                 # log가 저장될 경로\n",
    "    do_train=True,                        # train 수행여부\n",
    "    do_eval=True,                        # eval 수행여부\n",
    "    eval_steps=1000\n",
    ")\n",
    "\n",
    "# model, tokenizer 생성\n",
    "model_name_or_path = 'bert-base-uncased'\n",
    "with training_args.strategy.scope():    # training_args가 영향을 미치는 model의 범위를 지정\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name_or_path,\n",
    "            from_pt=bool(\".bin\" in model_name_or_path),\n",
    "        )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name_or_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-quantum",
   "metadata": {},
   "source": [
    "#### 위와 같이 Huggingface 프레임워크 구조에 따라 Model과 Tokenizer를 간단히 생성\n",
    "- model 생성 시에 training_args의 scope 안에서 진행했다는 것이 눈에 띕니다.\n",
    "- <MARK> with 구문을 생략하면 TFTrainer에 전달하고픈 옵션이 제대로 전달되지 않아 결과적으로 모델이 오동작하게 되는 경우가 생길 수 있습니다.</MARK>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "valued-gateway",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load pre-computed DatasetInfo (eg: splits, num examples,...) from GCS: glue/mrpc/1.0.0\n",
      "INFO:absl:Load dataset info from /tmp/tmp8w85fevctfds\n",
      "INFO:absl:Field info.description from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.config_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.config_description from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.location from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Generating dataset glue (/aiffel/tensorflow_datasets/glue/mrpc/1.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 1.43 MiB (download: 1.43 MiB, generated: Unknown size, total: 1.43 MiB) to /aiffel/tensorflow_datasets/glue/mrpc/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf6637dbb9894cc8b9c175b5b6fcbae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6188b183222e490cb2e001b221c3efeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Downloading https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2Fmrpc_dev_ids.tsv?alt=media&token=ec5c0836-31d5-48f4-b431-7480817f1adc into /aiffel/tensorflow_datasets/downloads/fire.goog.com_v0_b_mtl-sent-repr.apps.com_o_2FjSIMlCiqs1QSmIykr4IRPnEHjPuGwAz5i40v8K9U0Z8.tsvalt=media&token=ec5c0836-31d5-48f4-b431-7480817f1adc.tmp.34222c4844794daaa4af743d41f5e08a...\n",
      "INFO:absl:Downloading https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt into /aiffel/tensorflow_datasets/downloads/dl.fbaip.com_sente_sente_msr_parap_test0PdekMcyqYR-w4Rx_d7OTryq0J3RlYRn4rAMajy9Mak.txt.tmp.b522661f4a764383ba52771aa256bb97...\n",
      "INFO:absl:Downloading https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt into /aiffel/tensorflow_datasets/downloads/dl.fbaip.com_sente_sente_msr_parap_trainfGxPZuQWGBti4Tbd1YNOwQr-OqxPejJ7gcp0Al6mlSk.txt.tmp.9038f932de4e4ecb9ab79a3f6f0be06f...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling glue-train.tfrecord...:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Done writing glue-train.tfrecord. Number of examples: 3668 (shards: [3668])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation examples...:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling glue-validation.tfrecord...:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Done writing glue-validation.tfrecord. Number of examples: 408 (shards: [408])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling glue-test.tfrecord...:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Done writing glue-test.tfrecord. Number of examples: 1725 (shards: [1725])\n",
      "INFO:absl:Constructing tf.data.Dataset for split None, from /aiffel/tensorflow_datasets/glue/mrpc/1.0.0\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/data/processors/glue.py:67: FutureWarning: This function will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"function\"), FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/data/processors/glue.py:175: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset glue downloaded and prepared to /aiffel/tensorflow_datasets/glue/mrpc/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.45723017939814814}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋 생성\n",
    "ds, info = tfds.load('glue/mrpc', with_info=True)\n",
    "train_dataset = glue_convert_examples_to_features(ds['train'], tokenizer, 128, 'mrpc')\n",
    "train_dataset = train_dataset.apply(tf.data.experimental.assert_cardinality(info.splits['train'].num_examples))\n",
    "\n",
    "# TFTrainer 생성\n",
    "trainer = TFTrainer(\n",
    "    model=model,                          # 학습시킬 model\n",
    "    args=training_args,                  # TFTrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=train_dataset,   # training dataset\n",
    ")\n",
    "\n",
    "# 학습 진행\n",
    "trainer.train()\n",
    "\n",
    "# 테스트\n",
    "test_dataset = glue_convert_examples_to_features(ds['test'], tokenizer, 128, 'mrpc')\n",
    "test_dataset = test_dataset.apply(tf.data.experimental.assert_cardinality(info.splits['test'].num_examples))\n",
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-dining",
   "metadata": {},
   "source": [
    "#### 이후 데이터셋을 생성하여, model, training_args과 함께 TFTrainer에 전달하는 것으로 학습을 위한 준비가 마무리됩니다. \n",
    "- 이후 trainer.train()을 호출하면 실제 학습이 진행됩니다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
